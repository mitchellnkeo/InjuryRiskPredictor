{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering - Injury Risk Predictor\n",
        "\n",
        "## Phase 3: Creating Predictive Features from Raw Training Data\n",
        "\n",
        "This notebook demonstrates the feature engineering pipeline:\n",
        "- Using the feature engineering module to create all features\n",
        "- Exploring engineered features and their distributions\n",
        "- Validating feature calculations\n",
        "- Preparing data for model training\n",
        "\n",
        "**Key Principle:** All features must avoid data leakage by only using past data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    HAS_SEABORN = True\n",
        "except ImportError:\n",
        "    HAS_SEABORN = False\n",
        "    print(\"Note: seaborn not available, using matplotlib only\")\n",
        "\n",
        "from src.ml.features import engineer_all_features, engineer_features_for_dataset\n",
        "from src.ml.preprocessing import (\n",
        "    handle_missing_values,\n",
        "    encode_categorical_features,\n",
        "    scale_features,\n",
        "    split_data_by_time,\n",
        "    create_feature_pipeline\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-darkgrid')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn-darkgrid')\n",
        "    except:\n",
        "        plt.style.use('default')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "training_logs = pd.read_csv('../data/training_logs.csv')\n",
        "\n",
        "print(f\"Loaded {len(training_logs)} rows of training data\")\n",
        "print(f\"Columns: {list(training_logs.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(training_logs.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Engineer Features for Sample Athlete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test feature engineering on a single athlete-week\n",
        "sample_athlete = training_logs['athlete_id'].iloc[0]\n",
        "sample_week = 12  # Mid-season week\n",
        "\n",
        "print(f\"Engineering features for athlete {sample_athlete}, week {sample_week}...\")\n",
        "features = engineer_all_features(training_logs, sample_athlete, sample_week)\n",
        "\n",
        "print(f\"\\nEngineered {len(features)} features:\")\n",
        "for key, value in features.items():\n",
        "    if isinstance(value, (int, float)):\n",
        "        print(f\"  {key}: {value:.3f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Engineer Features for Entire Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engineer features for all athlete-week combinations\n",
        "print(\"Engineering features for entire dataset...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "feature_df = engineer_features_for_dataset(training_logs)\n",
        "\n",
        "print(f\"\\n✓ Feature engineering complete!\")\n",
        "print(f\"  Rows: {len(feature_df)}\")\n",
        "print(f\"  Columns: {len(feature_df.columns)}\")\n",
        "print(f\"\\nFeature columns:\")\n",
        "print(list(feature_df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample of engineered features\n",
        "print(\"Sample of engineered features:\")\n",
        "print(feature_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Explore Engineered Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics for key features\n",
        "key_features = ['acwr', 'monotony', 'strain', 'week_over_week_change', \n",
        "                'acwr_trend', 'weeks_above_threshold', 'distance_from_baseline']\n",
        "\n",
        "print(\"Summary Statistics for Key Features:\")\n",
        "print(feature_df[key_features].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of key features by injury status\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# ACWR\n",
        "injured_mask = feature_df['injured'] == True\n",
        "axes[0, 0].hist(feature_df[~injured_mask]['acwr'], bins=50, alpha=0.7, \n",
        "                label='Not Injured', color='green', density=True)\n",
        "axes[0, 0].hist(feature_df[injured_mask]['acwr'], bins=50, alpha=0.7, \n",
        "                label='Injured', color='red', density=True)\n",
        "axes[0, 0].set_xlabel('ACWR')\n",
        "axes[0, 0].set_ylabel('Density')\n",
        "axes[0, 0].set_title('ACWR Distribution by Injury Status')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Monotony\n",
        "axes[0, 1].hist(feature_df[~injured_mask]['monotony'], bins=50, alpha=0.7, \n",
        "                label='Not Injured', color='green', density=True)\n",
        "axes[0, 1].hist(feature_df[injured_mask]['monotony'], bins=50, alpha=0.7, \n",
        "                label='Injured', color='red', density=True)\n",
        "axes[0, 1].set_xlabel('Monotony')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].set_title('Monotony Distribution by Injury Status')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Weeks Above Threshold\n",
        "axes[1, 0].hist(feature_df[~injured_mask]['weeks_above_threshold'], bins=10, alpha=0.7, \n",
        "                label='Not Injured', color='green', density=True)\n",
        "axes[1, 0].hist(feature_df[injured_mask]['weeks_above_threshold'], bins=10, alpha=0.7, \n",
        "                label='Injured', color='red', density=True)\n",
        "axes[1, 0].set_xlabel('Weeks Above Threshold (ACWR > 1.3)')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Weeks Above Threshold by Injury Status')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# ACWR Trend\n",
        "axes[1, 1].hist(feature_df[~injured_mask]['acwr_trend'], bins=50, alpha=0.7, \n",
        "                label='Not Injured', color='green', density=True)\n",
        "axes[1, 1].hist(feature_df[injured_mask]['acwr_trend'], bins=50, alpha=0.7, \n",
        "                label='Injured', color='red', density=True)\n",
        "axes[1, 1].set_xlabel('ACWR Trend (Slope)')\n",
        "axes[1, 1].set_ylabel('Density')\n",
        "axes[1, 1].set_title('ACWR Trend by Injury Status')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../outputs/engineered_features_distributions.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation with injury status\n",
        "numeric_features = feature_df.select_dtypes(include=[np.number]).columns\n",
        "numeric_features = [f for f in numeric_features if f not in ['athlete_id', 'week', 'injured']]\n",
        "\n",
        "correlations = feature_df[numeric_features + ['injured']].corr()['injured'].sort_values(ascending=False)\n",
        "\n",
        "print(\"Feature Correlations with Injury Status:\")\n",
        "print(\"=\" * 60)\n",
        "for feature, corr in correlations.items():\n",
        "    if feature != 'injured':\n",
        "        print(f\"{feature:30s}: {corr:7.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply preprocessing pipeline\n",
        "print(\"Applying preprocessing pipeline...\")\n",
        "\n",
        "# Handle missing values\n",
        "feature_df_clean = handle_missing_values(feature_df, method='forward_fill')\n",
        "print(f\"✓ Missing values handled\")\n",
        "\n",
        "# Encode categorical features\n",
        "feature_df_encoded, encoders = encode_categorical_features(feature_df_clean)\n",
        "print(f\"✓ Categorical features encoded: {list(encoders.keys())}\")\n",
        "\n",
        "# Scale features\n",
        "feature_df_scaled, scaler = scale_features(feature_df_encoded, scaler_type='standard', fit=True)\n",
        "print(f\"✓ Features scaled using StandardScaler\")\n",
        "\n",
        "print(f\"\\nFinal feature matrix shape: {feature_df_scaled.shape}\")\n",
        "print(f\"Ready for model training!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Time-Based Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data by time to avoid data leakage\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = split_data_by_time(\n",
        "    feature_df_scaled,\n",
        "    train_weeks=(1, 14),\n",
        "    val_weeks=(15, 19),\n",
        "    test_weeks=(20, 24)\n",
        ")\n",
        "\n",
        "print(\"Time-Based Data Split:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Training set:   {len(X_train):5d} samples ({y_train.sum()} injuries, {y_train.mean()*100:.1f}%)\")\n",
        "print(f\"Validation set: {len(X_val):5d} samples ({y_val.sum()} injuries, {y_val.mean()*100:.1f}%)\")\n",
        "print(f\"Test set:       {len(X_test):5d} samples ({y_test.sum()} injuries, {y_test.mean()*100:.1f}%)\")\n",
        "print(f\"\\nTotal features: {X_train.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Processed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save engineered features for model training\n",
        "feature_df_scaled.to_csv('../data/engineered_features.csv', index=False)\n",
        "print(\"✓ Saved engineered features to data/engineered_features.csv\")\n",
        "\n",
        "# Save train/val/test splits\n",
        "X_train.to_csv('../data/X_train.csv', index=False)\n",
        "y_train.to_csv('../data/y_train.csv', index=False)\n",
        "X_val.to_csv('../data/X_val.csv', index=False)\n",
        "y_val.to_csv('../data/y_val.csv', index=False)\n",
        "X_test.to_csv('../data/X_test.csv', index=False)\n",
        "y_test.to_csv('../data/y_test.csv', index=False)\n",
        "\n",
        "print(\"✓ Saved train/validation/test splits\")\n",
        "print(\"\\nData is ready for model training in Phase 4!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### Features Engineered:\n",
        "1. **Core Metrics**: ACWR, monotony, strain, week-over-week change\n",
        "2. **Derived Features**: ACWR trend, weeks above threshold, distance from baseline\n",
        "3. **Lag Features**: Previous week ACWR, 2 weeks ago ACWR\n",
        "4. **Athlete-Specific**: Age groups, experience levels, baseline fitness\n",
        "5. **Temporal Features**: Recent injury history\n",
        "\n",
        "### Key Validations:\n",
        "- ✓ All features calculated correctly\n",
        "- ✓ No data leakage (only past data used)\n",
        "- ✓ Features show correlation with injury status\n",
        "- ✓ Data split by time (not randomly)\n",
        "- ✓ Preprocessing pipeline applied\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to Phase 4: Model Development\n",
        "- Train baseline and ML models\n",
        "- Evaluate model performance"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
