{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training - Injury Risk Predictor\n",
    "\n",
    "This notebook trains and evaluates multiple models for injury risk prediction.\n",
    "\n",
    "## Steps:\n",
    "1. Load processed data\n",
    "2. Split data by time (train/validation/test)\n",
    "3. Train baseline rule-based model\n",
    "4. Train ML models (Logistic Regression, Random Forest, XGBoost)\n",
    "5. Evaluate and compare models\n",
    "6. Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: XGBoost not available. Install with: pip install xgboost\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.ml.features import engineer_features_for_dataset\n",
    "from src.ml.preprocessing import (\n",
    "    handle_missing_values,\n",
    "    encode_categorical_features,\n",
    "    split_data_by_time,\n",
    "    scale_features,\n",
    "    prepare_features_for_training\n",
    ")\n",
    "from src.ml.train import train_all_models, save_model\n",
    "from src.ml.evaluate import (\n",
    "    evaluate_model,\n",
    "    compare_models,\n",
    "    create_confusion_matrix,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve\n",
    ")\n",
    "from src.ml.models import get_all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logs shape: (3600, 16)\n",
      "\n",
      "Injury rate: 16.94%\n"
     ]
    }
   ],
   "source": [
    "# Load training logs (metadata already included)\n",
    "training_logs = pd.read_csv('../data/training_logs.csv')\n",
    "\n",
    "print(f\"Training logs shape: {training_logs.shape}\")\n",
    "print(f\"\\nInjury rate: {training_logs['injured'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after feature engineering: (3600, 20)\n",
      "\n",
      "Feature columns:\n",
      "['acute_load', 'chronic_load', 'acwr', 'monotony', 'strain', 'week_over_week_change', 'acwr_trend', 'weeks_above_threshold', 'distance_from_baseline', 'previous_week_acwr', 'two_weeks_ago_acwr', 'recent_injury', 'age', 'age_group', 'experience_years', 'experience_level', 'baseline_weekly_miles', 'athlete_id', 'week', 'injured']\n"
     ]
    }
   ],
   "source": [
    "# Engineer all features\n",
    "df = engineer_features_for_dataset(training_logs)\n",
    "\n",
    "print(f\"Data shape after feature engineering: {df.shape}\")\n",
    "print(f\"\\nFeature columns:\")\n",
    "print(df.columns.tolist()[:20])  # Show first 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after preprocessing: (3600, 20)\n",
      "\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values\n",
    "df = handle_missing_values(df, method='forward_fill')\n",
    "\n",
    "# Encode categorical features\n",
    "df, encoders = encode_categorical_features(df)\n",
    "\n",
    "print(f\"Data shape after preprocessing: {df.shape}\")\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-Based Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 2100 samples, 14.67% injury rate\n",
      "Validation set: 750 samples, 20.53% injury rate\n",
      "Test set: 750 samples, 19.73% injury rate\n"
     ]
    }
   ],
   "source": [
    "# Split by time to avoid data leakage\n",
    "# Train: weeks 1-14 (60%)\n",
    "# Validation: weeks 15-19 (20%)\n",
    "# Test: weeks 20-24 (20%)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_data_by_time(\n",
    "    df,\n",
    "    train_weeks=(1, 14),\n",
    "    val_weeks=(15, 19),\n",
    "    test_weeks=(20, 24)\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {y_train.mean():.2%} injury rate\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples, {y_val.mean():.2%} injury rate\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {y_test.mean():.2%} injury rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled successfully\n",
      "Training features shape: (2100, 17)\n"
     ]
    }
   ],
   "source": [
    "# Scale features (fit on training, transform all)\n",
    "X_train_scaled, scaler = scale_features(X_train, fit=True, scaler_type='standard')\n",
    "X_val_scaled, _ = scale_features(X_val, fit=False, scaler=scaler)\n",
    "X_test_scaled, _ = scale_features(X_test, fit=False, scaler=scaler)\n",
    "\n",
    "print(f\"Features scaled successfully\")\n",
    "print(f\"Training features shape: {X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Baseline Model...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Evaluation Report: Baseline\n",
      "============================================================\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.812\n",
      "  Precision: 0.562\n",
      "  Recall:    0.383\n",
      "  F1-Score:  0.456\n",
      "  ROC-AUC:   0.653\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Injured       0.85      0.92      0.89       596\n",
      "     Injured       0.56      0.38      0.46       154\n",
      "\n",
      "    accuracy                           0.81       750\n",
      "   macro avg       0.71      0.65      0.67       750\n",
      "weighted avg       0.79      0.81      0.80       750\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Not Inj  Injured\n",
      "Actual Not Inj    550      46\n",
      "       Injured      95      59\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Training Logistic Regression...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Evaluation Report: Logistic Regression\n",
      "============================================================\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.715\n",
      "  Precision: 0.408\n",
      "  Recall:    0.864\n",
      "  F1-Score:  0.554\n",
      "  ROC-AUC:   0.835\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Injured       0.95      0.68      0.79       596\n",
      "     Injured       0.41      0.86      0.55       154\n",
      "\n",
      "    accuracy                           0.71       750\n",
      "   macro avg       0.68      0.77      0.67       750\n",
      "weighted avg       0.84      0.71      0.74       750\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Not Inj  Injured\n",
      "Actual Not Inj    403     193\n",
      "       Injured      21     133\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "Evaluation Report: Random Forest\n",
      "============================================================\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.763\n",
      "  Precision: 0.459\n",
      "  Recall:    0.877\n",
      "  F1-Score:  0.603\n",
      "  ROC-AUC:   0.857\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Injured       0.96      0.73      0.83       596\n",
      "     Injured       0.46      0.88      0.60       154\n",
      "\n",
      "    accuracy                           0.76       750\n",
      "   macro avg       0.71      0.80      0.72       750\n",
      "weighted avg       0.86      0.76      0.78       750\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Not Inj  Injured\n",
      "Actual Not Inj    437     159\n",
      "       Injured      19     135\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Model Comparison (Validation Set)\n",
      "============================================================\n",
      "              Model  Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
      "      Random Forest     0.763      0.459   0.877     0.603    0.857\n",
      "Logistic Regression     0.715      0.408   0.864     0.554    0.835\n",
      "           Baseline     0.812      0.562   0.383     0.456    0.653\n",
      "\n",
      "============================================================\n",
      "Best Model Evaluation on Test Set\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Evaluation Report: Random Forest (Test)\n",
      "============================================================\n",
      "\n",
      "Metrics:\n",
      "  Accuracy:  0.827\n",
      "  Precision: 0.539\n",
      "  Recall:    0.838\n",
      "  F1-Score:  0.656\n",
      "  ROC-AUC:   0.906\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Injured       0.95      0.82      0.88       602\n",
      "     Injured       0.54      0.84      0.66       148\n",
      "\n",
      "    accuracy                           0.83       750\n",
      "   macro avg       0.75      0.83      0.77       750\n",
      "weighted avg       0.87      0.83      0.84       750\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "              Not Inj  Injured\n",
      "Actual Not Inj    496     106\n",
      "       Injured      24     124\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train all models (baseline + ML models)\n",
    "# Set tune_hyperparameters=True for hyperparameter tuning (slower but better)\n",
    "results = train_all_models(\n",
    "    X_train_scaled, y_train,\n",
    "    X_val_scaled, y_val,\n",
    "    X_test_scaled, y_test,\n",
    "    tune_hyperparameters=False  # Set to True for better performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create comparison table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comparison_metrics \u001b[38;5;241m=\u001b[39m {name: metrics \u001b[38;5;28;01mfor\u001b[39;00m name, (_, metrics) \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems() \n\u001b[1;32m      3\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_name\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      4\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m compare_models(comparison_metrics)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Comparison (Validation Set):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create comparison table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m comparison_metrics \u001b[38;5;241m=\u001b[39m {name: metrics \u001b[38;5;28;01mfor\u001b[39;00m name, (_, metrics) \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems() \n\u001b[1;32m      3\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_name\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[1;32m      4\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m compare_models(comparison_metrics)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Comparison (Validation Set):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "comparison_metrics = {name: metrics for name, (_, metrics) in results.items() \n",
    "                     if name != 'best_model' and name != 'best_model_name'}\n",
    "comparison_df = compare_models(comparison_metrics)\n",
    "\n",
    "print(\"\\nModel Comparison (Validation Set):\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model\n",
    "best_model_name = results['best_model_name']\n",
    "best_model = results['best_model'][0]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "y_test_proba = best_model.predict_proba(X_test_scaled)\n",
    "\n",
    "create_confusion_matrix(y_test, y_test_pred, best_model_name, \n",
    "                       save_path='../models/confusion_matrix.png')\n",
    "print(\"✓ Confusion matrix saved\")\n",
    "\n",
    "# ROC Curve\n",
    "plot_roc_curve(y_test, y_test_proba, best_model_name,\n",
    "              save_path='../models/roc_curve.png')\n",
    "print(\"✓ ROC curve saved\")\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plot_precision_recall_curve(y_test, y_test_proba, best_model_name,\n",
    "                           save_path='../models/pr_curve.png')\n",
    "print(\"✓ Precision-Recall curve saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train_scaled.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features:\")\n",
    "    print(feature_importance.head(15).to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_features = feature_importance.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"✓ Feature importance plot saved\")\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # For linear models, use coefficients\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_train_scaled.columns,\n",
    "        'coefficient': best_model.coef_[0]\n",
    "    }).sort_values('coefficient', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 15 Most Important Features (by coefficient magnitude):\")\n",
    "    print(feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model and scaler\n",
    "save_model(best_model, scaler, best_model_name, output_dir='../models')\n",
    "\n",
    "print(f\"\\n✓ Model training complete!\")\n",
    "print(f\"✓ Best model ({best_model_name}) saved to models/\")\n",
    "print(f\"✓ Scaler saved to models/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
