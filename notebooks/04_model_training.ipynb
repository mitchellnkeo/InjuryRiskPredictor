{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training - Injury Risk Predictor\n",
        "\n",
        "This notebook trains and evaluates multiple models for injury risk prediction.\n",
        "\n",
        "## Steps:\n",
        "1. Load processed data\n",
        "2. Split data by time (train/validation/test)\n",
        "3. Train baseline rule-based model\n",
        "4. Train ML models (Logistic Regression, Random Forest, XGBoost)\n",
        "5. Evaluate and compare models\n",
        "6. Save best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from src.ml.features import engineer_features_for_dataset\n",
        "from src.ml.preprocessing import (\n",
        "    handle_missing_values,\n",
        "    encode_categorical_features,\n",
        "    split_data_by_time,\n",
        "    scale_features,\n",
        "    prepare_features_for_training\n",
        ")\n",
        "from src.ml.train import train_all_models, save_model\n",
        "from src.ml.evaluate import (\n",
        "    evaluate_model,\n",
        "    compare_models,\n",
        "    create_confusion_matrix,\n",
        "    plot_roc_curve,\n",
        "    plot_precision_recall_curve\n",
        ")\n",
        "from src.ml.models import get_all_models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training logs and athlete metadata\n",
        "training_logs = pd.read_csv('../data/training_logs.csv')\n",
        "athlete_metadata = pd.read_csv('../data/athlete_metadata.csv')\n",
        "\n",
        "print(f\"Training logs shape: {training_logs.shape}\")\n",
        "print(f\"Athlete metadata shape: {athlete_metadata.shape}\")\n",
        "print(f\"\\nInjury rate: {training_logs['injured'].mean():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Engineer Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Engineer all features\n",
        "df = engineer_features_for_dataset(training_logs, athlete_metadata)\n",
        "\n",
        "print(f\"Data shape after feature engineering: {df.shape}\")\n",
        "print(f\"\\nFeature columns:\")\n",
        "print(df.columns.tolist()[:20])  # Show first 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "df = handle_missing_values(df, method='forward_fill')\n",
        "\n",
        "# Encode categorical features\n",
        "df, encoders = encode_categorical_features(df)\n",
        "\n",
        "print(f\"Data shape after preprocessing: {df.shape}\")\n",
        "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Time-Based Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split by time to avoid data leakage\n",
        "# Train: weeks 1-14 (60%)\n",
        "# Validation: weeks 15-19 (20%)\n",
        "# Test: weeks 20-24 (20%)\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = split_data_by_time(\n",
        "    df,\n",
        "    train_weeks=(1, 14),\n",
        "    val_weeks=(15, 19),\n",
        "    test_weeks=(20, 24)\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples, {y_train.mean():.2%} injury rate\")\n",
        "print(f\"Validation set: {X_val.shape[0]} samples, {y_val.mean():.2%} injury rate\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples, {y_test.mean():.2%} injury rate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Scale Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale features (fit on training, transform all)\n",
        "X_train_scaled, scaler = scale_features(X_train, fit=True, scaler_type='standard')\n",
        "X_val_scaled, _ = scale_features(X_val, fit=False, scaler=scaler)\n",
        "X_test_scaled, _ = scale_features(X_test, fit=False, scaler=scaler)\n",
        "\n",
        "print(f\"Features scaled successfully\")\n",
        "print(f\"Training features shape: {X_train_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train All Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train all models (baseline + ML models)\n",
        "# Set tune_hyperparameters=True for hyperparameter tuning (slower but better)\n",
        "results = train_all_models(\n",
        "    X_train_scaled, y_train,\n",
        "    X_val_scaled, y_val,\n",
        "    X_test_scaled, y_test,\n",
        "    tune_hyperparameters=False  # Set to True for better performance\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_metrics = {name: metrics for name, (_, metrics) in results.items() \n",
        "                     if name != 'best_model' and name != 'best_model_name'}\n",
        "comparison_df = compare_models(comparison_metrics)\n",
        "\n",
        "print(\"\\nModel Comparison (Validation Set):\")\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get best model\n",
        "best_model_name = results['best_model_name']\n",
        "best_model = results['best_model'][0]\n",
        "\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "y_test_pred = best_model.predict(X_test_scaled)\n",
        "y_test_proba = best_model.predict_proba(X_test_scaled)\n",
        "\n",
        "create_confusion_matrix(y_test, y_test_pred, best_model_name, \n",
        "                       save_path='../models/confusion_matrix.png')\n",
        "print(\"✓ Confusion matrix saved\")\n",
        "\n",
        "# ROC Curve\n",
        "plot_roc_curve(y_test, y_test_proba, best_model_name,\n",
        "              save_path='../models/roc_curve.png')\n",
        "print(\"✓ ROC curve saved\")\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plot_precision_recall_curve(y_test, y_test_proba, best_model_name,\n",
        "                           save_path='../models/pr_curve.png')\n",
        "print(\"✓ Precision-Recall curve saved\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance (for tree-based models)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train_scaled.columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 15 Most Important Features:\")\n",
        "    print(feature_importance.head(15).to_string(index=False))\n",
        "    \n",
        "    # Plot feature importance\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    top_features = feature_importance.head(15)\n",
        "    plt.barh(range(len(top_features)), top_features['importance'])\n",
        "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
        "    plt.xlabel('Importance')\n",
        "    plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../models/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Feature importance plot saved\")\n",
        "elif hasattr(best_model, 'coef_'):\n",
        "    # For linear models, use coefficients\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train_scaled.columns,\n",
        "        'coefficient': best_model.coef_[0]\n",
        "    }).sort_values('coefficient', key=abs, ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 15 Most Important Features (by coefficient magnitude):\")\n",
        "    print(feature_importance.head(15).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Best Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save best model and scaler\n",
        "save_model(best_model, scaler, best_model_name, output_dir='../models')\n",
        "\n",
        "print(f\"\\n✓ Model training complete!\")\n",
        "print(f\"✓ Best model ({best_model_name}) saved to models/\")\n",
        "print(f\"✓ Scaler saved to models/\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
