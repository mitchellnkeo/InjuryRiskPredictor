# Daily Build Journal - January 13, 2026

**Status:** ✅ In Progress  
**Phase:** Phase 4 - Model Development  
**Completion Marker:** [START] → [END]

---

## Commit #1: Model Training Infrastructure

### What Was Done
Created the complete model training pipeline including:
- `src/ml/models.py` - Model definitions (baseline + ML models)
- `src/ml/evaluate.py` - Evaluation metrics and visualizations
- `src/ml/train.py` - Training pipeline and model persistence
- `notebooks/04_model_training.ipynb` - Interactive training notebook
- `scripts/test_training.py` - Quick validation script

### Key Takeaways

#### 1. **Baseline Model First (Rule-Based)**
- **What:** Created a simple rule-based model that predicts injury risk based solely on ACWR thresholds (ACWR > 1.5 = high risk)
- **Why:** In ML, you always need a baseline to compare against. If your fancy ML model can't beat a simple rule, something's wrong!
- **For Junior SWE:** Think of it like this - before building a complex recommendation system, you'd first test "just recommend the most popular items" as a baseline. Same concept here. The baseline gives us a performance floor - our ML models MUST beat this.

#### 2. **Multiple Models, Not Just One**
- **What:** Implemented three different ML models: Logistic Regression, Random Forest, and XGBoost
- **Why:** Different models have different strengths:
  - **Logistic Regression:** Simple, interpretable, fast. Good starting point.
  - **Random Forest:** Handles non-linear relationships, feature interactions automatically
  - **XGBoost:** Often best performance, but more complex and slower to train
- **For Junior SWE:** This is like trying different tools for a job. You wouldn't use a hammer for everything - sometimes you need a screwdriver. Same with ML models. We try multiple and pick the best one for our specific problem.

#### 3. **Evaluation Metrics Matter More Than Accuracy**
- **What:** Implemented multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC
- **Why:** For injury prediction, we care MORE about Recall (catching actual injuries) than Precision (avoiding false alarms). Missing an injury is worse than a false alarm!
- **For Junior SWE:** Imagine a spam filter:
  - **High Precision, Low Recall:** Very few false positives (good emails marked spam), but misses lots of spam
  - **High Recall, Low Precision:** Catches most spam, but also marks some good emails as spam
  - For injury prediction, we want HIGH RECALL - we'd rather warn someone unnecessarily than miss a real injury risk.

#### 4. **Time-Based Data Splitting (Critical!)**
- **What:** Split data by time periods (weeks 1-14 train, 15-19 val, 20-24 test) instead of randomly
- **Why:** In time-series problems, random splitting causes "data leakage" - you'd be training on future data to predict the past, which doesn't work in real life!
- **For Junior SWE:** Think of it like predicting stock prices. If you randomly split your data, you might train on data from 2025 to predict 2024 prices. That's cheating! In real life, you only know past data. So we split chronologically - train on past, validate on recent past, test on future.

#### 5. **Feature Scaling Before Training**
- **What:** Standardized all features using StandardScaler (mean=0, std=1)
- **Why:** ML models (especially Logistic Regression) are sensitive to feature scales. ACWR might be 0.8-2.0, while weekly_load might be 10-50. Without scaling, the model thinks weekly_load is more important just because it's bigger!
- **For Junior SWE:** It's like comparing apples and oranges. You need to normalize them first. Scaling puts all features on the same "playing field" so the model can learn which features actually matter, not which ones happen to have bigger numbers.

#### 6. **Model Persistence (Save/Load)**
- **What:** Implemented `save_model()` and `load_model()` functions using pickle
- **Why:** Training takes time! Once we train a good model, we save it so we can use it later without retraining. This is essential for deployment.
- **For Junior SWE:** Like saving your game progress. You don't want to retrain the model every time you want to make a prediction. Save it once, load it whenever you need it.

### Interview Explanation

**Q: "Walk me through your model training process."**

**A:** "I started with a baseline rule-based model using ACWR thresholds from sports science research. This gives us a performance floor - our ML models must beat this simple heuristic.

Then I implemented three ML models: Logistic Regression for interpretability, Random Forest for handling non-linear relationships, and XGBoost for best performance. I split the data chronologically (weeks 1-14 train, 15-19 val, 20-24 test) to avoid data leakage - we can't use future data to predict the past in real applications.

I scaled all features using StandardScaler because models are sensitive to feature scales. Then I trained each model and evaluated using multiple metrics, prioritizing Recall since missing an injury is worse than a false alarm.

Finally, I saved the best-performing model and scaler for deployment. The entire pipeline is modular - each component (preprocessing, training, evaluation) is separate, making it easy to test and maintain."

**Q: "Why did you use multiple models instead of just picking one?"**

**A:** "Different models have different strengths. Logistic Regression is simple and interpretable - I can see which features matter most. Random Forest handles complex interactions automatically. XGBoost often gives best performance but is harder to interpret.

By comparing all three, I can choose the best model for this specific problem. If XGBoost only slightly outperforms Logistic Regression, I might choose Logistic Regression for its interpretability. If there's a big performance gap, I'll choose XGBoost.

This is standard practice in ML - try multiple approaches and pick the best one based on your specific constraints (performance, interpretability, speed, etc.)."

**Q: "Why is Recall more important than Precision for injury prediction?"**

**A:** "In injury prediction, the cost of a false negative (missing an injury) is much higher than a false positive (false alarm). If we predict someone is low risk but they're actually high risk, they might get injured. That's bad!

A false positive means we warn someone unnecessarily - they might reduce training load, but that's much better than getting injured. So we optimize for Recall - catching as many actual injuries as possible, even if it means some false alarms.

This is domain-specific. For spam detection, you might prefer high precision (don't mark important emails as spam). For medical diagnosis, you want high recall (don't miss diseases). Context matters!"

---

## Commit #2: Model Evaluation & Visualization

### What Was Done
Created comprehensive evaluation module with:
- Confusion matrix visualization
- ROC curve plotting
- Precision-Recall curve plotting
- Model comparison tables
- Feature importance analysis

### Key Takeaways

#### 1. **Confusion Matrix - The Truth Table**
- **What:** A 2x2 table showing True Positives, False Positives, True Negatives, False Negatives
- **Why:** Accuracy alone is misleading! If only 10% of people get injured, a model that always predicts "no injury" would be 90% accurate but useless.
- **For Junior SWE:** The confusion matrix shows WHERE your model makes mistakes:
  - **True Positives:** Correctly predicted injuries (good!)
  - **False Positives:** Predicted injury but didn't happen (false alarm - okay for our use case)
  - **False Negatives:** Missed an injury (BAD - this is what we want to minimize)
  - **True Negatives:** Correctly predicted no injury (good!)

#### 2. **ROC Curve - Overall Model Performance**
- **What:** Plots True Positive Rate (Recall) vs False Positive Rate across different thresholds
- **Why:** Shows how well the model separates injured vs non-injured athletes across all possible thresholds
- **For Junior SWE:** The ROC curve answers: "If I change my prediction threshold, how does performance change?" The area under the curve (AUC) is a single number summarizing overall performance. AUC = 1.0 is perfect, AUC = 0.5 is random guessing.

#### 3. **Precision-Recall Curve - For Imbalanced Data**
- **What:** Plots Precision vs Recall across different thresholds
- **Why:** When classes are imbalanced (few injuries vs many non-injuries), Precision-Recall curve is more informative than ROC
- **For Junior SWE:** ROC can be misleading with imbalanced data. If 90% of people don't get injured, you can get high ROC-AUC by just predicting "no injury" most of the time. Precision-Recall focuses on the minority class (injuries) which is what we care about.

#### 4. **Feature Importance - What Actually Matters**
- **What:** For tree-based models (Random Forest, XGBoost), we can see which features the model uses most
- **Why:** Validates our domain knowledge - ACWR should be important! Also helps identify useless features we can remove.
- **For Junior SWE:** This is like debugging - "Why did the model make this prediction?" Feature importance shows which features drive the model's decisions. If age is more important than ACWR, something's wrong (ACWR should be #1 based on research).

### Interview Explanation

**Q: "How do you evaluate your models beyond accuracy?"**

**A:** "I use multiple evaluation metrics because accuracy alone is misleading, especially with imbalanced data. I create a confusion matrix to see where mistakes happen - are we missing injuries (false negatives) or giving false alarms (false positives)?

I plot ROC curves to see overall discrimination ability, and Precision-Recall curves which are better for imbalanced data. I prioritize Recall because missing an injury is worse than a false alarm.

I also analyze feature importance to validate domain knowledge - ACWR should be the most important feature based on sports science research. If it's not, I investigate why."

---

## Commit #3: Training Pipeline & Hyperparameter Tuning

### What Was Done
Created complete training pipeline with:
- Automated model training for all models
- Hyperparameter tuning support (GridSearchCV/RandomizedSearchCV)
- Model comparison and selection
- Best model saving

### Key Takeaways

#### 1. **Hyperparameter Tuning - Finding the Best Settings**
- **What:** Automatically tests different combinations of model parameters (e.g., tree depth, learning rate)
- **Why:** Default parameters are rarely optimal. Tuning can significantly improve performance.
- **For Junior SWE:** Think of it like tuning a car engine. The default settings work, but you can optimize for speed, fuel efficiency, etc. Same with ML models - we tune parameters to optimize for our specific problem (high recall in our case).

#### 2. **GridSearchCV vs RandomizedSearchCV**
- **GridSearchCV:** Tests ALL combinations of parameters (exhaustive but slow)
- **RandomizedSearchCV:** Tests random combinations (faster, often finds good enough solution)
- **Why:** For large parameter spaces, exhaustive search is too slow. Randomized search is a good trade-off.
- **For Junior SWE:** Like searching for a restaurant:
  - **GridSearch:** Check every single restaurant in the city (thorough but slow)
  - **RandomizedSearch:** Check 20 random restaurants (faster, probably find something good)

#### 3. **Train/Validation/Test Split - Why Three Sets?**
- **Train:** Model learns from this data
- **Validation:** Used to tune hyperparameters and select best model
- **Test:** Final evaluation - only touched once, at the very end!
- **Why:** If we tune hyperparameters on test set, we're "cheating" - we're optimizing for test performance, which doesn't generalize.
- **For Junior SWE:** Like studying for a test:
  - **Train:** Your study materials (you learn from these)
  - **Validation:** Practice exams (you use these to figure out what to study more)
  - **Test:** The actual exam (you only see it once, at the end. If you study the test questions, you're cheating!)

### Interview Explanation

**Q: "How do you prevent overfitting?"**

**A:** "I use multiple techniques:
1. **Time-based splitting:** Prevents data leakage - can't use future to predict past
2. **Train/Val/Test separation:** Never touch test set until final evaluation
3. **Cross-validation:** For hyperparameter tuning, I use CV to get more robust estimates
4. **Compare train vs validation performance:** If train accuracy is much higher than validation, we're overfitting
5. **Regularization:** Models like Logistic Regression and XGBoost have built-in regularization parameters

The key is to always evaluate on data the model hasn't seen during training or tuning."

---

## Commit #4: Bug Fix - Function Signature

### What Was Done
Fixed `engineer_features_for_dataset()` function call - it only takes one argument (training_logs), not two.

### Key Takeaways

#### 1. **Read the Function Signature!**
- **What:** The function signature shows `engineer_features_for_dataset(df: pd.DataFrame)` - only one argument
- **Why:** I assumed it needed athlete_metadata separately, but the training_logs DataFrame already contains all athlete metadata columns merged in
- **For Junior SWE:** Always check function signatures before calling them! Read the docstring or source code. Don't assume - verify.

#### 2. **Data Already Preprocessed**
- **What:** The training_logs.csv already has athlete metadata (age, experience_years, baseline_weekly_miles) merged in
- **Why:** This was done during data generation - the generator creates one unified dataset
- **For Junior SWE:** Understand your data pipeline! Know what each step produces. The feature engineering function expects unified data, not separate DataFrames.

### Interview Explanation

**Q: "Tell me about a bug you fixed."**

**A:** "I was calling `engineer_features_for_dataset()` with two arguments, but the function only accepts one. The training logs DataFrame already contains all athlete metadata merged in from the data generation step.

I fixed it by reading the function signature and understanding the data pipeline - the generator creates a unified dataset, so the feature engineering function expects that format. This taught me to always verify function signatures and understand data flow before making assumptions."

---

## Day Summary

### What We Accomplished
✅ Created complete model training infrastructure  
✅ Implemented baseline rule-based model  
✅ Implemented three ML models (Logistic Regression, Random Forest, XGBoost)  
✅ Created comprehensive evaluation system  
✅ Built training pipeline with hyperparameter tuning support  
✅ Created interactive training notebook  
✅ Fixed function call bug  

### Key Learnings for Junior SWE

1. **Always Start with a Baseline**
   - Simple rule-based model gives you a performance floor
   - If ML can't beat simple rules, something's wrong

2. **Multiple Models, Multiple Metrics**
   - Try different models - they have different strengths
   - Use multiple metrics - accuracy alone is misleading
   - Prioritize metrics based on business needs (Recall for injury prediction)

3. **Time-Based Splitting is Critical**
   - Never randomly split time-series data
   - Train on past, validate on recent past, test on future
   - Prevents data leakage

4. **Feature Scaling Matters**
   - ML models are sensitive to feature scales
   - Always scale features before training
   - StandardScaler (mean=0, std=1) is a good default

5. **Evaluation is More Than Accuracy**
   - Confusion matrix shows WHERE mistakes happen
   - ROC curve shows overall discrimination
   - Precision-Recall curve better for imbalanced data
   - Feature importance validates domain knowledge

6. **Model Persistence is Essential**
   - Save trained models - don't retrain every time
   - Save scalers too - need them for new predictions
   - Use pickle for Python objects

### Fundamental Knowledge Notes

#### Machine Learning Pipeline
1. **Data Preparation:** Load, clean, engineer features
2. **Preprocessing:** Handle missing values, encode categories, scale features
3. **Data Splitting:** Train/Validation/Test (by time for time-series)
4. **Model Training:** Train multiple models, tune hyperparameters
5. **Evaluation:** Multiple metrics, visualizations, feature importance
6. **Model Selection:** Pick best model based on performance + constraints
7. **Persistence:** Save model and preprocessing objects

#### Model Types
- **Baseline:** Simple rule-based (no ML) - establishes performance floor
- **Logistic Regression:** Linear model, interpretable, fast
- **Random Forest:** Ensemble of trees, handles non-linear relationships
- **XGBoost:** Gradient boosting, often best performance

#### Evaluation Metrics
- **Accuracy:** Overall correctness (misleading for imbalanced data)
- **Precision:** Of predicted positives, how many are actually positive?
- **Recall:** Of actual positives, how many did we catch?
- **F1-Score:** Harmonic mean of Precision and Recall
- **ROC-AUC:** Area under ROC curve (overall discrimination ability)

#### Key Concepts
- **Data Leakage:** Using future information to predict past (BAD!)
- **Overfitting:** Model memorizes training data, doesn't generalize
- **Class Imbalance:** One class much more common than another
- **Feature Scaling:** Normalizing features to same scale
- **Hyperparameter Tuning:** Finding best model settings
- **Cross-Validation:** Robust evaluation using multiple train/test splits

#### Best Practices
- Always split data BEFORE any preprocessing (fit scaler on train, transform all)
- Never touch test set until final evaluation
- Use time-based splitting for time-series data
- Try multiple models and metrics
- Save models and preprocessing objects
- Document everything!

---

**Date Started:** January 13, 2026  
**Phases Completed:** Phase 4 (Model Development)  
**Total Commits Documented:** 4

---

**Completion Marker:** [END]
