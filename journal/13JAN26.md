# Daily Build Journal - January 13, 2026

**Status:** ✅ In Progress  
**Phase:** Phase 4 - Model Development  
**Completion Marker:** [START] → [END]

---

## Commit #1: Model Training Infrastructure

### What Was Done
Created the complete model training pipeline including:
- `src/ml/models.py` - Model definitions (baseline + ML models)
- `src/ml/evaluate.py` - Evaluation metrics and visualizations
- `src/ml/train.py` - Training pipeline and model persistence
- `notebooks/04_model_training.ipynb` - Interactive training notebook
- `scripts/test_training.py` - Quick validation script

### Key Takeaways

#### 1. **Baseline Model First (Rule-Based)**
- **What:** Created a simple rule-based model that predicts injury risk based solely on ACWR thresholds (ACWR > 1.5 = high risk)
- **Why:** In ML, you always need a baseline to compare against. If your fancy ML model can't beat a simple rule, something's wrong!
- **For Junior SWE:** Think of it like this - before building a complex recommendation system, you'd first test "just recommend the most popular items" as a baseline. Same concept here. The baseline gives us a performance floor - our ML models MUST beat this.

#### 2. **Multiple Models, Not Just One**
- **What:** Implemented three different ML models: Logistic Regression, Random Forest, and XGBoost
- **Why:** Different models have different strengths:
  - **Logistic Regression:** Simple, interpretable, fast. Good starting point.
  - **Random Forest:** Handles non-linear relationships, feature interactions automatically
  - **XGBoost:** Often best performance, but more complex and slower to train
- **For Junior SWE:** This is like trying different tools for a job. You wouldn't use a hammer for everything - sometimes you need a screwdriver. Same with ML models. We try multiple and pick the best one for our specific problem.

#### 3. **Evaluation Metrics Matter More Than Accuracy**
- **What:** Implemented multiple metrics: Accuracy, Precision, Recall, F1-Score, ROC-AUC
- **Why:** For injury prediction, we care MORE about Recall (catching actual injuries) than Precision (avoiding false alarms). Missing an injury is worse than a false alarm!
- **For Junior SWE:** Imagine a spam filter:
  - **High Precision, Low Recall:** Very few false positives (good emails marked spam), but misses lots of spam
  - **High Recall, Low Precision:** Catches most spam, but also marks some good emails as spam
  - For injury prediction, we want HIGH RECALL - we'd rather warn someone unnecessarily than miss a real injury risk.

#### 4. **Time-Based Data Splitting (Critical!)**
- **What:** Split data by time periods (weeks 1-14 train, 15-19 val, 20-24 test) instead of randomly
- **Why:** In time-series problems, random splitting causes "data leakage" - you'd be training on future data to predict the past, which doesn't work in real life!
- **For Junior SWE:** Think of it like predicting stock prices. If you randomly split your data, you might train on data from 2025 to predict 2024 prices. That's cheating! In real life, you only know past data. So we split chronologically - train on past, validate on recent past, test on future.

#### 5. **Feature Scaling Before Training**
- **What:** Standardized all features using StandardScaler (mean=0, std=1)
- **Why:** ML models (especially Logistic Regression) are sensitive to feature scales. ACWR might be 0.8-2.0, while weekly_load might be 10-50. Without scaling, the model thinks weekly_load is more important just because it's bigger!
- **For Junior SWE:** It's like comparing apples and oranges. You need to normalize them first. Scaling puts all features on the same "playing field" so the model can learn which features actually matter, not which ones happen to have bigger numbers.

#### 6. **Model Persistence (Save/Load)**
- **What:** Implemented `save_model()` and `load_model()` functions using pickle
- **Why:** Training takes time! Once we train a good model, we save it so we can use it later without retraining. This is essential for deployment.
- **For Junior SWE:** Like saving your game progress. You don't want to retrain the model every time you want to make a prediction. Save it once, load it whenever you need it.

### Interview Explanation

**Q: "Walk me through your model training process."**

**A:** "I started with a baseline rule-based model using ACWR thresholds from sports science research. This gives us a performance floor - our ML models must beat this simple heuristic.

Then I implemented three ML models: Logistic Regression for interpretability, Random Forest for handling non-linear relationships, and XGBoost for best performance. I split the data chronologically (weeks 1-14 train, 15-19 val, 20-24 test) to avoid data leakage - we can't use future data to predict the past in real applications.

I scaled all features using StandardScaler because models are sensitive to feature scales. Then I trained each model and evaluated using multiple metrics, prioritizing Recall since missing an injury is worse than a false alarm.

Finally, I saved the best-performing model and scaler for deployment. The entire pipeline is modular - each component (preprocessing, training, evaluation) is separate, making it easy to test and maintain."

**Q: "Why did you use multiple models instead of just picking one?"**

**A:** "Different models have different strengths. Logistic Regression is simple and interpretable - I can see which features matter most. Random Forest handles complex interactions automatically. XGBoost often gives best performance but is harder to interpret.

By comparing all three, I can choose the best model for this specific problem. If XGBoost only slightly outperforms Logistic Regression, I might choose Logistic Regression for its interpretability. If there's a big performance gap, I'll choose XGBoost.

This is standard practice in ML - try multiple approaches and pick the best one based on your specific constraints (performance, interpretability, speed, etc.)."

**Q: "Why is Recall more important than Precision for injury prediction?"**

**A:** "In injury prediction, the cost of a false negative (missing an injury) is much higher than a false positive (false alarm). If we predict someone is low risk but they're actually high risk, they might get injured. That's bad!

A false positive means we warn someone unnecessarily - they might reduce training load, but that's much better than getting injured. So we optimize for Recall - catching as many actual injuries as possible, even if it means some false alarms.

This is domain-specific. For spam detection, you might prefer high precision (don't mark important emails as spam). For medical diagnosis, you want high recall (don't miss diseases). Context matters!"

---

## Commit #2: Model Evaluation & Visualization

### What Was Done
Created comprehensive evaluation module with:
- Confusion matrix visualization
- ROC curve plotting
- Precision-Recall curve plotting
- Model comparison tables
- Feature importance analysis

### Key Takeaways

#### 1. **Confusion Matrix - The Truth Table**
- **What:** A 2x2 table showing True Positives, False Positives, True Negatives, False Negatives
- **Why:** Accuracy alone is misleading! If only 10% of people get injured, a model that always predicts "no injury" would be 90% accurate but useless.
- **For Junior SWE:** The confusion matrix shows WHERE your model makes mistakes:
  - **True Positives:** Correctly predicted injuries (good!)
  - **False Positives:** Predicted injury but didn't happen (false alarm - okay for our use case)
  - **False Negatives:** Missed an injury (BAD - this is what we want to minimize)
  - **True Negatives:** Correctly predicted no injury (good!)

#### 2. **ROC Curve - Overall Model Performance**
- **What:** Plots True Positive Rate (Recall) vs False Positive Rate across different thresholds
- **Why:** Shows how well the model separates injured vs non-injured athletes across all possible thresholds
- **For Junior SWE:** The ROC curve answers: "If I change my prediction threshold, how does performance change?" The area under the curve (AUC) is a single number summarizing overall performance. AUC = 1.0 is perfect, AUC = 0.5 is random guessing.

#### 3. **Precision-Recall Curve - For Imbalanced Data**
- **What:** Plots Precision vs Recall across different thresholds
- **Why:** When classes are imbalanced (few injuries vs many non-injuries), Precision-Recall curve is more informative than ROC
- **For Junior SWE:** ROC can be misleading with imbalanced data. If 90% of people don't get injured, you can get high ROC-AUC by just predicting "no injury" most of the time. Precision-Recall focuses on the minority class (injuries) which is what we care about.

#### 4. **Feature Importance - What Actually Matters**
- **What:** For tree-based models (Random Forest, XGBoost), we can see which features the model uses most
- **Why:** Validates our domain knowledge - ACWR should be important! Also helps identify useless features we can remove.
- **For Junior SWE:** This is like debugging - "Why did the model make this prediction?" Feature importance shows which features drive the model's decisions. If age is more important than ACWR, something's wrong (ACWR should be #1 based on research).

### Interview Explanation

**Q: "How do you evaluate your models beyond accuracy?"**

**A:** "I use multiple evaluation metrics because accuracy alone is misleading, especially with imbalanced data. I create a confusion matrix to see where mistakes happen - are we missing injuries (false negatives) or giving false alarms (false positives)?

I plot ROC curves to see overall discrimination ability, and Precision-Recall curves which are better for imbalanced data. I prioritize Recall because missing an injury is worse than a false alarm.

I also analyze feature importance to validate domain knowledge - ACWR should be the most important feature based on sports science research. If it's not, I investigate why."

---

## Commit #3: Training Pipeline & Hyperparameter Tuning

### What Was Done
Created complete training pipeline with:
- Automated model training for all models
- Hyperparameter tuning support (GridSearchCV/RandomizedSearchCV)
- Model comparison and selection
- Best model saving

### Key Takeaways

#### 1. **Hyperparameter Tuning - Finding the Best Settings**
- **What:** Automatically tests different combinations of model parameters (e.g., tree depth, learning rate)
- **Why:** Default parameters are rarely optimal. Tuning can significantly improve performance.
- **For Junior SWE:** Think of it like tuning a car engine. The default settings work, but you can optimize for speed, fuel efficiency, etc. Same with ML models - we tune parameters to optimize for our specific problem (high recall in our case).

#### 2. **GridSearchCV vs RandomizedSearchCV**
- **GridSearchCV:** Tests ALL combinations of parameters (exhaustive but slow)
- **RandomizedSearchCV:** Tests random combinations (faster, often finds good enough solution)
- **Why:** For large parameter spaces, exhaustive search is too slow. Randomized search is a good trade-off.
- **For Junior SWE:** Like searching for a restaurant:
  - **GridSearch:** Check every single restaurant in the city (thorough but slow)
  - **RandomizedSearch:** Check 20 random restaurants (faster, probably find something good)

#### 3. **Train/Validation/Test Split - Why Three Sets?**
- **Train:** Model learns from this data
- **Validation:** Used to tune hyperparameters and select best model
- **Test:** Final evaluation - only touched once, at the very end!
- **Why:** If we tune hyperparameters on test set, we're "cheating" - we're optimizing for test performance, which doesn't generalize.
- **For Junior SWE:** Like studying for a test:
  - **Train:** Your study materials (you learn from these)
  - **Validation:** Practice exams (you use these to figure out what to study more)
  - **Test:** The actual exam (you only see it once, at the end. If you study the test questions, you're cheating!)

### Interview Explanation

**Q: "How do you prevent overfitting?"**

**A:** "I use multiple techniques:
1. **Time-based splitting:** Prevents data leakage - can't use future to predict past
2. **Train/Val/Test separation:** Never touch test set until final evaluation
3. **Cross-validation:** For hyperparameter tuning, I use CV to get more robust estimates
4. **Compare train vs validation performance:** If train accuracy is much higher than validation, we're overfitting
5. **Regularization:** Models like Logistic Regression and XGBoost have built-in regularization parameters

The key is to always evaluate on data the model hasn't seen during training or tuning."

---

## Commit #4: Bug Fix - Function Signature

### What Was Done
Fixed `engineer_features_for_dataset()` function call - it only takes one argument (training_logs), not two.

### Key Takeaways

#### 1. **Read the Function Signature!**
- **What:** The function signature shows `engineer_features_for_dataset(df: pd.DataFrame)` - only one argument
- **Why:** I assumed it needed athlete_metadata separately, but the training_logs DataFrame already contains all athlete metadata columns merged in
- **For Junior SWE:** Always check function signatures before calling them! Read the docstring or source code. Don't assume - verify.

#### 2. **Data Already Preprocessed**
- **What:** The training_logs.csv already has athlete metadata (age, experience_years, baseline_weekly_miles) merged in
- **Why:** This was done during data generation - the generator creates one unified dataset
- **For Junior SWE:** Understand your data pipeline! Know what each step produces. The feature engineering function expects unified data, not separate DataFrames.

### Interview Explanation

**Q: "Tell me about a bug you fixed."**

**A:** "I was calling `engineer_features_for_dataset()` with two arguments, but the function only accepts one. The training logs DataFrame already contains all athlete metadata merged in from the data generation step.

I fixed it by reading the function signature and understanding the data pipeline - the generator creates a unified dataset, so the feature engineering function expects that format. This taught me to always verify function signatures and understand data flow before making assumptions."

---

## Day Summary

### What We Accomplished
✅ Created complete model training infrastructure  
✅ Implemented baseline rule-based model  
✅ Implemented three ML models (Logistic Regression, Random Forest, XGBoost)  
✅ Created comprehensive evaluation system  
✅ Built training pipeline with hyperparameter tuning support  
✅ Created interactive training notebook  
✅ Fixed function call bug  

### Key Learnings for Junior SWE

1. **Always Start with a Baseline**
   - Simple rule-based model gives you a performance floor
   - If ML can't beat simple rules, something's wrong

2. **Multiple Models, Multiple Metrics**
   - Try different models - they have different strengths
   - Use multiple metrics - accuracy alone is misleading
   - Prioritize metrics based on business needs (Recall for injury prediction)

3. **Time-Based Splitting is Critical**
   - Never randomly split time-series data
   - Train on past, validate on recent past, test on future
   - Prevents data leakage

4. **Feature Scaling Matters**
   - ML models are sensitive to feature scales
   - Always scale features before training
   - StandardScaler (mean=0, std=1) is a good default

5. **Evaluation is More Than Accuracy**
   - Confusion matrix shows WHERE mistakes happen
   - ROC curve shows overall discrimination
   - Precision-Recall curve better for imbalanced data
   - Feature importance validates domain knowledge

6. **Model Persistence is Essential**
   - Save trained models - don't retrain every time
   - Save scalers too - need them for new predictions
   - Use pickle for Python objects

### Fundamental Knowledge Notes

#### Machine Learning Pipeline
1. **Data Preparation:** Load, clean, engineer features
2. **Preprocessing:** Handle missing values, encode categories, scale features
3. **Data Splitting:** Train/Validation/Test (by time for time-series)
4. **Model Training:** Train multiple models, tune hyperparameters
5. **Evaluation:** Multiple metrics, visualizations, feature importance
6. **Model Selection:** Pick best model based on performance + constraints
7. **Persistence:** Save model and preprocessing objects

#### Model Types
- **Baseline:** Simple rule-based (no ML) - establishes performance floor
- **Logistic Regression:** Linear model, interpretable, fast
- **Random Forest:** Ensemble of trees, handles non-linear relationships
- **XGBoost:** Gradient boosting, often best performance

#### Evaluation Metrics
- **Accuracy:** Overall correctness (misleading for imbalanced data)
- **Precision:** Of predicted positives, how many are actually positive?
- **Recall:** Of actual positives, how many did we catch?
- **F1-Score:** Harmonic mean of Precision and Recall
- **ROC-AUC:** Area under ROC curve (overall discrimination ability)

#### Key Concepts
- **Data Leakage:** Using future information to predict past (BAD!)
- **Overfitting:** Model memorizes training data, doesn't generalize
- **Class Imbalance:** One class much more common than another
- **Feature Scaling:** Normalizing features to same scale
- **Hyperparameter Tuning:** Finding best model settings
- **Cross-Validation:** Robust evaluation using multiple train/test splits

#### Best Practices
- Always split data BEFORE any preprocessing (fit scaler on train, transform all)
- Never touch test set until final evaluation
- Use time-based splitting for time-series data
- Try multiple models and metrics
- Save models and preprocessing objects
- Document everything!

---

## Commit #5: Model Interpretation & Validation Infrastructure

### What Was Done
Created comprehensive model interpretation module and notebook:
- `src/ml/interpretation.py` - Interpretation utilities (feature importance, SHAP, partial dependence, error analysis, research validation)
- `notebooks/05_model_interpretation.ipynb` - Interactive interpretation notebook
- `scripts/test_interpretation.py` - Test script for interpretation module
- `outputs/` directory for saving plots and visualizations

### Key Takeaways

#### 1. **Model Interpretability is Critical**
- **What:** Created tools to understand WHY the model makes predictions, not just WHAT it predicts
- **Why:** In healthcare/sports applications, you need to explain predictions to users. "The model says high risk" isn't enough - you need to say WHY (e.g., "ACWR is 1.6, which is above the 1.5 threshold")
- **For Junior SWE:** Think of it like a doctor explaining a diagnosis. They don't just say "you're sick" - they explain the symptoms and test results that led to that conclusion. Same with ML models - interpretability builds trust and helps users make informed decisions.

#### 2. **Feature Importance - What Actually Matters**
- **What:** Visualizes which features the model uses most to make predictions
- **Why:** Validates our domain knowledge - ACWR should be #1 or #2 feature. If it's not, something's wrong with our features or model!
- **For Junior SWE:** This is like debugging - "Why did the model predict high risk?" Feature importance shows which features drive the decision. If age is more important than ACWR, that's suspicious (ACWR should be #1 based on research).

#### 3. **SHAP Values - Explaining Individual Predictions**
- **What:** SHAP (SHapley Additive exPlanations) shows how each feature contributes to each prediction
- **Why:** Feature importance shows overall patterns, but SHAP shows WHY a specific athlete got a high-risk prediction
- **For Junior SWE:** Feature importance is like "ACWR is important overall." SHAP is like "For THIS athlete, ACWR contributed +0.3 to their risk score, strain contributed +0.2, etc." It's personalized explanation.

#### 4. **Partial Dependence Plots - Understanding Feature Effects**
- **What:** Shows how injury risk changes as we vary one feature while keeping others constant
- **Why:** Validates that the model learned the right relationships - risk should increase above ACWR 1.3 and especially 1.5
- **For Junior SWE:** It's like a controlled experiment. "If we only change ACWR, how does risk change?" The plot should show risk increasing above 1.3 (matches research). If it doesn't, the model didn't learn correctly.

#### 5. **Error Analysis - Learning from Mistakes**
- **What:** Analyzes which cases the model gets wrong (false positives vs false negatives)
- **Why:** Helps identify patterns - are we missing injuries in specific scenarios? Are false alarms happening for certain athlete types?
- **For Junior SWE:** Like reviewing test mistakes to study better. "What do false negatives have in common? Maybe athletes with high ACWR but low strain?" This helps improve the model.

#### 6. **Research Validation - Ensuring Model Makes Sense**
- **What:** Automated checks that model predictions align with sports science research
- **Why:** ML models can learn spurious patterns. We need to verify they learned the RIGHT patterns (ACWR > 1.3 = higher risk, etc.)
- **For Junior SWE:** This is like fact-checking. The model might predict high risk for low ACWR (wrong!) or low risk for high ACWR (also wrong!). Research validation catches these issues before deployment.

### Interview Explanation

**Q: "How do you ensure your model's predictions are trustworthy and explainable?"**

**A:** "I built a comprehensive interpretation pipeline with multiple techniques:

First, I analyze feature importance to validate domain knowledge - ACWR should be the top feature based on research. If it's not, I investigate why.

I use SHAP values to explain individual predictions - showing exactly how each feature contributed to a specific athlete's risk score. This allows me to say 'Your ACWR of 1.6 contributed +0.3 to your risk score' rather than just 'you're high risk.'

I create partial dependence plots to verify the model learned correct relationships - risk should increase above ACWR 1.3 and especially 1.5, matching research findings.

I perform error analysis to identify patterns in mistakes - are we missing injuries in specific scenarios? This helps improve the model.

Finally, I have automated research validation that checks the model aligns with sports science - does it predict higher risk at ACWR > 1.3? Does it penalize rapid load spikes? These checks ensure the model learned the right patterns, not spurious correlations.

This multi-layered approach builds trust and ensures the model is both accurate and explainable."

**Q: "What's the difference between feature importance and SHAP values?"**

**A:** "Feature importance shows which features matter MOST OVERALL - like 'ACWR is the #1 most important feature across all predictions.' It's a global view.

SHAP values show how each feature contributes to EACH INDIVIDUAL prediction - like 'For this specific athlete, ACWR contributed +0.3, strain contributed +0.2, age contributed -0.1.' It's a local, personalized explanation.

Think of it like weather forecasting:
- Feature importance: 'Temperature is the most important factor for predicting rain overall'
- SHAP: 'For today's forecast, temperature contributed +0.4 to rain probability, humidity contributed +0.3, wind contributed -0.2'

Both are useful - feature importance validates our domain knowledge, SHAP explains individual predictions to users."

**Q: "How do you validate that your model learned the right patterns?"**

**A:** "I use automated research validation that checks the model aligns with sports science findings:

1. **ACWR Thresholds:** Does the model predict higher risk at ACWR > 1.3? Does it predict much higher risk at ACWR > 1.5? These are research-established thresholds.

2. **Load Spikes:** Does the model penalize rapid week-over-week increases (>20%)? Research shows sudden spikes increase injury risk.

3. **Strain Relationship:** Does higher strain predict higher risk? This should be a positive relationship.

4. **Feature Importance:** Is ACWR in the top 3 features? If not, something's wrong - it should be #1 based on research.

I also use partial dependence plots to visualize these relationships - if the plot shows risk decreasing as ACWR increases, that's wrong! The plot should match research expectations.

This validation catches cases where the model learned spurious correlations instead of real patterns. It's like fact-checking the model's 'knowledge' against established research."

---

## Commit #6: Interpretation Notebook Testing & Refinement

### What Was Done
- Updated interpretation notebook to handle missing models gracefully
- Added fallback model loading logic
- Created test script for interpretation module
- Verified all interpretation functions work correctly

### Key Takeaways

#### 1. **Graceful Error Handling**
- **What:** Notebook handles missing models/scalers without crashing
- **Why:** Users might run interpretation before training, or models might be in different locations
- **For Junior SWE:** Always handle edge cases! Don't assume files exist. Use try/except and provide helpful error messages. "File not found" is bad UX - "No model found. Train one first using..." is better.

#### 2. **Flexible Model Loading**
- **What:** Tries multiple model name formats and file locations
- **Why:** Different training scripts might save models with different names
- **For Junior SWE:** Make your code resilient to variations. Don't assume exact naming conventions. Try multiple options and provide fallbacks.

### Interview Explanation

**Q: "Tell me about a time you improved error handling."**

**A:** "When building the interpretation notebook, I initially assumed models would always exist. But users might run interpretation before training, or models might be saved with different names.

I added graceful error handling:
1. Try to load scaler - if not found, create a new one from training data
2. Try multiple model name formats (random_forest, Random Forest, random_forest_model)
3. If all fail, scan the models directory and load any .pkl file
4. Provide clear error messages with next steps

This makes the notebook more user-friendly - instead of crashing with 'FileNotFoundError', it explains what's missing and how to fix it. Better UX = better code!"

---

## Day Summary

### What We Accomplished
✅ Created complete model interpretation infrastructure  
✅ Implemented feature importance visualization  
✅ Added SHAP values support for explainability  
✅ Created partial dependence plots  
✅ Built error analysis tools  
✅ Implemented research validation checks  
✅ Created interactive interpretation notebook  
✅ Tested and verified all interpretation functions work  

### Key Learnings for Junior SWE

1. **Interpretability is Non-Negotiable**
   - Users need to understand WHY predictions are made
   - Multiple techniques (importance, SHAP, partial dependence) provide different insights
   - Research validation ensures model learned correct patterns

2. **Feature Importance Validates Domain Knowledge**
   - ACWR should be top feature - if not, investigate!
   - Feature importance catches when models learn wrong patterns
   - Use it to debug and improve models

3. **SHAP Values Explain Individual Predictions**
   - Global (importance) vs Local (SHAP) explanations
   - SHAP shows personalized contributions to each prediction
   - Critical for user trust and understanding

4. **Partial Dependence Shows Feature Effects**
   - Visualizes how risk changes with each feature
   - Validates model learned correct relationships
   - Should match research expectations (ACWR > 1.3 = higher risk)

5. **Error Analysis Identifies Improvement Opportunities**
   - Analyze false positives vs false negatives
   - Find patterns in mistakes
   - Use insights to improve model

6. **Research Validation is Essential**
   - Automated checks ensure model aligns with domain knowledge
   - Catches spurious correlations
   - Prevents deploying wrong models

### Fundamental Knowledge Notes

#### Model Interpretability Techniques
1. **Feature Importance:**
   - Global view: which features matter most overall
   - Tree-based: feature_importances_ attribute
   - Linear: coefficient magnitudes
   - Use: Validate domain knowledge, identify important features

2. **SHAP Values:**
   - Local view: feature contributions to individual predictions
   - Based on game theory (Shapley values)
   - Shows: "Feature X contributed +0.3 to this prediction"
   - Use: Explain individual predictions to users

3. **Partial Dependence Plots:**
   - Shows: How predictions change as one feature varies
   - Method: Vary feature, average predictions, plot
   - Use: Understand feature effects, validate relationships

4. **Error Analysis:**
   - Analyzes: False positives, false negatives
   - Compares: Feature distributions for errors vs correct predictions
   - Use: Identify improvement opportunities

5. **Research Validation:**
   - Checks: Model predictions align with domain knowledge
   - Examples: ACWR > 1.3 = higher risk, load spikes increase risk
   - Use: Ensure model learned correct patterns

#### Best Practices
- Always validate feature importance matches domain knowledge
- Use multiple interpretation techniques (they provide different insights)
- Validate against research/domain expertise
- Explain predictions to users (not just provide scores)
- Analyze errors to improve models
- Document interpretation findings

---

## Commit #7: Notebook Fixes & Feature Importance Explanation

### What Was Done
- Fixed import path error in notebook 03 (`sys.path.append('..')`)
- Fixed visualization display issues (plots now show inline in notebooks)
- Created comprehensive explanation for ACWR ranking (#3 vs #1)
- Created pre-Phase 6 testing checklist
- Verified all notebooks run successfully

### Key Takeaways

#### 1. **Import Path Consistency Matters**
- **What:** Notebook 03 had incorrect import path setup
- **Why:** Different notebooks used different path methods, causing confusion
- **Fix:** Standardized all notebooks to use `sys.path.append('..')`
- **For Junior SWE:** Consistency is key! When you have multiple files doing similar things, use the same pattern. Makes debugging easier and code more maintainable.

#### 2. **Visualization Display in Notebooks**
- **What:** Plots were being saved but not displayed inline
- **Why:** `plt.close()` was called before `plt.show()`
- **Fix:** Added `show=True` parameter (default) to plotting functions, only close if `show=False`
- **For Junior SWE:** In Jupyter notebooks, you want to SEE your plots, not just save them. Always use `plt.show()` for notebooks, `plt.close()` for scripts that run headless.

#### 3. **Feature Importance Interpretation**
- **What:** ACWR ranked #3, week_over_week_change ranked #1
- **Why:** This is actually CORRECT! Both features are highly important, and week_over_week_change captures sudden spikes immediately
- **Key Insight:** ACWR needs 2+ weeks to build up, but week_over_week_change flags danger immediately
- **For Junior SWE:** Don't assume #1 is always "best" - context matters! Both features are in top 3, which validates domain knowledge. If ACWR was #10, THAT would be a problem. #3 is excellent!

#### 4. **Research Validation**
- **What:** Model learned that sudden spikes (week_over_week_change) are slightly more predictive than sustained high load (ACWR)
- **Why:** Matches research - sudden spikes are a major injury risk factor
- **Validation:** Both features work together - when both are high, risk is highest (80% probability)
- **For Junior SWE:** When your model learns something unexpected, investigate! In this case, it's actually correct and aligns with research. Always validate against domain knowledge.

### Interview Explanation

**Q: "You mentioned ACWR should be the most important feature, but it's ranked #3. Is that a problem?"**

**A:** "Not at all! This is actually correct and validates our domain knowledge. ACWR is ranked #3, which is excellent - it's still highly important. Week-over-week change is #1, which makes sense because:

1. **Sudden spikes are immediately dangerous** - A 75% week-over-week increase flags danger right away
2. **ACWR needs time to build** - It requires 2+ weeks of high load to reach dangerous levels
3. **Both features work together** - When both are high, injury risk is highest (80% probability in our data)

Looking at our data generation logic, injuries are created based on:
- High ACWR for 2+ weeks → 60% probability
- Week-over-week spike >20% → 40% probability  
- BOTH conditions → 80% probability

The model correctly learned that sudden spikes are slightly more predictive in our dataset, which aligns with research showing sudden load increases are a major injury risk factor. ACWR being in the top 3 validates our domain knowledge - if it was ranked #10, THAT would be a problem!"

**Q: "How do you debug when feature importance doesn't match expectations?"**

**A:** "First, I validate it's actually a problem. In this case, ACWR #3 is fine - it's still highly important.

If it WAS a problem (e.g., ACWR ranked #20), I'd:
1. **Check feature engineering** - Is ACWR calculated correctly?
2. **Check data quality** - Are there enough high ACWR examples?
3. **Check feature scaling** - Are features on similar scales?
4. **Check correlations** - Is ACWR highly correlated with another feature (multicollinearity)?
5. **Check model type** - Some models handle features differently
6. **Validate against research** - Does the model still predict higher risk at ACWR > 1.3?

The key is to investigate systematically, not panic. Sometimes the model learns something better than expected!"

---

## Commit #8: Pre-Phase 6 Testing & Documentation

### What Was Done
- Created comprehensive testing checklist
- Created pre-Phase 6 readiness checklist
- Verified all infrastructure components
- Documented known issues (non-blocking)

### Key Takeaways

#### 1. **Testing Before Major Milestones**
- **What:** Created checklists before starting API development
- **Why:** Ensures all prerequisites are met before building on top
- **For Junior SWE:** Always verify foundations before building! If notebooks don't work, the API won't work either. Test incrementally.

#### 2. **Documentation Saves Time**
- **What:** Created checklists and explanations for future reference
- **Why:** When you come back to this project later, you'll remember what was done and why
- **For Junior SWE:** Document as you go! Future you will thank present you. Also helps teammates understand your decisions.

#### 3. **Non-Blocking Issues**
- **What:** Identified known issues that don't prevent progress
- **Examples:** Segmentation fault in terminal (notebooks work fine), optional dependencies
- **Why:** Don't let perfect be the enemy of good - move forward on what works
- **For Junior SWE:** Not every issue needs to be fixed immediately. Document it, understand impact, prioritize. Some issues can wait.

### Interview Explanation

**Q: "How do you ensure you're ready to move to the next phase of development?"**

**A:** "I create a comprehensive checklist that verifies:
1. **Infrastructure works** - All modules import, data files exist, models load
2. **Previous phases complete** - All notebooks run, features engineered, model trained
3. **Prerequisites met** - Everything needed for next phase is ready
4. **Known issues documented** - Non-blocking issues are noted but don't prevent progress

I test systematically - run each notebook end-to-end, verify model can make predictions, check all outputs are generated. Only when everything passes do I move forward. This prevents building on shaky foundations and catching issues early."

---

## Day Summary

### What We Accomplished
✅ Created complete model training infrastructure  
✅ Implemented baseline and ML models  
✅ Built comprehensive evaluation system  
✅ Created model interpretation tools  
✅ Fixed notebook import and visualization issues  
✅ Explained feature importance rankings  
✅ Created testing checklists  
✅ Verified readiness for Phase 6  

### Key Learnings for Junior SWE

1. **Consistency Matters**
   - Use same patterns across similar files
   - Makes debugging easier
   - Improves maintainability

2. **Visualization Display**
   - Notebooks need `plt.show()` to display plots
   - Scripts can use `plt.close()` to save memory
   - Always test visualizations work

3. **Feature Importance Interpretation**
   - Top 3 is excellent (not just #1)
   - Context matters - validate against domain knowledge
   - Unexpected rankings might be correct - investigate!

4. **Testing Before Milestones**
   - Verify foundations before building on top
   - Create checklists for major transitions
   - Test incrementally

5. **Documentation**
   - Document as you go
   - Explain decisions and rationale
   - Helps future you and teammates

6. **Non-Blocking Issues**
   - Not every issue needs immediate fix
   - Document and prioritize
   - Move forward on what works

### Fundamental Knowledge Notes

#### Notebook Best Practices
- **Import paths:** Use `sys.path.append('..')` consistently
- **Visualizations:** Use `plt.show()` in notebooks, `plt.close()` in scripts
- **Error handling:** Graceful fallbacks for optional dependencies
- **Testing:** Run notebooks end-to-end before moving forward

#### Feature Importance
- **Top 3 is excellent** - Don't obsess over #1
- **Validate against domain knowledge** - Does ranking make sense?
- **Context matters** - Unexpected rankings might be correct
- **Investigate systematically** - Check engineering, data, scaling, correlations

#### Project Management
- **Checklists before milestones** - Verify prerequisites
- **Document decisions** - Future you will thank you
- **Prioritize issues** - Not everything needs immediate fix
- **Test incrementally** - Catch issues early

---

**Date Started:** January 13, 2026  
**Phases Completed:** Phase 4 (Model Development), Phase 5 (Model Interpretation & Validation)  
**Total Commits Documented:** 8

---

**Completion Marker:** [END]
