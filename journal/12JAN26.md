# Build Journal - January 12, 2026

**Status:** ✅ COMPLETE - Ready for Google Drive Transfer

## Date
January 12, 2026

## Summary
Completed Phase 0 (Research & Planning) by creating comprehensive documentation for the Injury Risk Predictor project, including research summaries, feature engineering specifications, and user flow diagrams. Started Phase 1 (Data Strategy & Generation) by building a synthetic data generator that creates realistic training datasets with injury labels based on sports science research.

---

## Commits & Pushes

### Commit #1: Complete Phase 0 - Research & Planning Documentation
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created `docs/RESEARCH.md` with summaries of 3 core research papers (Gabbett 2016, Hulin 2014, Soligard 2016)
- Documented ACWR methodology, core metrics (ACWR, Monotony, Strain, Week-over-Week Change)
- Created `docs/FEATURES.md` with 13-15 features to engineer, calculation methods, and validation requirements
- Created `docs/USER_FLOW.md` with user journey diagrams, page structures, and error handling flows
- Established foundation for ML problem definition (binary classification: high risk vs low/moderate risk)

#### Key Takeaways
- ACWR (Acute:Chronic Workload Ratio) is the primary injury predictor - when athletes increase training volume too quickly relative to their baseline, injury risk increases 2-4x
- The "sweet spot" for training is ACWR between 0.8-1.3 - this represents optimal progressive overload without excessive risk
- Synthetic data generation is the right approach for MVP - it's faster, controllable, and allows us to cite research parameters
- Feature engineering requires careful attention to data leakage - must only use past data to predict future injuries

#### Interview Explanation
**Question:** "Can you explain the research foundation and approach for your Injury Risk Predictor project?"

**Answer:** 
"The project is based on peer-reviewed sports science research, specifically the Acute:Chronic Workload Ratio (ACWR) concept established by Gabbett in 2016. The core insight is that injury risk increases dramatically when athletes spike their training volume too quickly relative to what they're adapted to. 

I implemented this by calculating ACWR as the ratio of acute load (last 7 days) to chronic load (last 28 days average). Research shows that ACWR between 0.8-1.3 is the 'sweet spot' with lowest injury risk, while values above 1.5 indicate 2-4x increased injury likelihood. 

For the MVP, I chose synthetic data generation over real data because it's faster, allows precise control over injury scenarios, and I can directly cite research parameters. The generator creates 150 athletes over 24 weeks with three training patterns: 30% safe progressive loading, 40% moderate spikes, and 30% aggressive spikes. Injuries are labeled probabilistically based on research: 60% chance if ACWR > 1.5 for 2+ weeks, 40% chance if week-over-week spike > 20%, and 80% chance if both conditions are met.

This approach ensures the dataset matches research distributions (15-30% injury rate) and allows us to validate that the model learns the same patterns that sports scientists have identified."

---

### Commit #3: Fix Missing Values in Data Generator
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Fixed missing values issue in generated CSV files (5,980 missing values)
- Changed `injury_type` from `None` to `'none'` for non-injured athletes
- Changed `injury_week` from `None` to `0` for non-injured athletes
- Added cleanup in `save_data()` method to ensure no NaN values remain
- Regenerated data and validated - all checks pass with no missing values

#### Key Takeaways
- Using `None` in pandas DataFrames creates NaN values which show up as missing data in CSV files
- For categorical data like `injury_type`, using a string value like `'none'` is better than `None`/NaN
- For numeric data like `injury_week`, using `0` as a sentinel value is clearer than NaN
- Always validate data quality after generation - the validation script caught this issue immediately
- Testing before committing is crucial - caught a data quality issue that would have caused problems later

#### Interview Explanation
**Question:** "How did you handle missing values in your synthetic data generation?"

**Answer:**
"Initially, I had a data quality issue where non-injured athletes had `None` values for `injury_type` and `injury_week`, which pandas converted to NaN and showed up as 5,980 missing values in the CSV. 

I fixed this by using explicit sentinel values: `'none'` for `injury_type` and `0` for `injury_week` when an athlete wasn't injured. This approach is cleaner because:
1. It eliminates missing values entirely - the dataset is complete
2. It's more interpretable - `'none'` clearly means no injury, `0` means no injury week
3. It's easier to work with in downstream analysis - no need to handle NaN cases

I also added validation in the `save_data()` method to ensure any remaining NaN values are filled before writing to CSV. The validation script now confirms zero missing values, which is important for machine learning pipelines that expect complete data.

This taught me the importance of testing data quality immediately after generation - catching issues early saves time and prevents problems in later phases."

---

### Commit #4: Complete Phase 2 - Exploratory Data Analysis
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created comprehensive EDA notebook (`notebooks/02_exploratory_analysis.ipynb`)
- Implemented data loading and inspection (missing values, data types, summary statistics)
- Created ACWR distribution analysis with histograms and box plots
- Built injury rate by ACWR zone visualization (bar chart with color coding)
- Implemented training load progression time series (injured vs safe athletes)
- Created correlation analysis with heatmap and scatter plots
- Implemented statistical tests (t-test for ACWR means, chi-square for ACWR zones)
- Added error handling for optional dependencies (seaborn fallback to matplotlib)
- Created outputs directory for saving visualizations
- Documented key insights and findings section

#### Key Takeaways
- EDA is crucial for validating synthetic data matches research expectations - confirmed injury rate (16.9%) is within 15-30% range
- ACWR distribution clearly shows injured athletes have higher ACWR (mean 1.60 vs 0.88) - validates the research foundation
- Statistical tests (t-test, chi-square) provide quantitative confirmation of relationships, not just visual inspection
- Error handling for optional dependencies (seaborn) makes notebooks more portable - users can run even without all packages
- Creating publication-quality visualizations (300 DPI) early helps with portfolio documentation
- Time series plots showing injured vs safe athletes are powerful for explaining the concept to non-technical audiences
- Correlation heatmaps reveal multicollinearity issues early (e.g., acute_load and weekly_load are highly correlated)

#### Interview Explanation
**Question:** "How did you validate your synthetic data and ensure it was suitable for machine learning?"

**Answer:**
"I performed comprehensive exploratory data analysis to validate the synthetic data matched research expectations and was ready for modeling. First, I checked data quality - confirmed no missing values, verified data types, and calculated summary statistics to ensure distributions were reasonable.

Then I validated the core research hypothesis: that high ACWR correlates with injury risk. I created visualizations showing ACWR distributions for injured vs non-injured athletes, which clearly showed injured athletes had higher ACWR values (mean 1.60 vs 0.88). I also calculated injury rates by ACWR zone and confirmed the 'sweet spot' (0.8-1.3) had the lowest injury rate, while high risk zones (>1.5) had significantly higher rates.

I performed statistical tests to quantify these relationships: a t-test confirmed the difference in ACWR means was statistically significant, and a chi-square test confirmed injury rates differed significantly across ACWR zones. This quantitative validation was important - not just visual inspection.

I also created correlation analysis to identify feature relationships and potential multicollinearity issues. Time series plots comparing injured vs safe athletes helped visualize how training load spikes precede injuries.

The EDA confirmed the data was ready for feature engineering - injury rates matched research (16.9% within 15-30% range), ACWR distributions were realistic, and clear predictive relationships existed. This validation step is critical when using synthetic data - you need to ensure it captures the real-world patterns you're trying to model."

---

### Commit #5: Complete Phase 3 - Feature Engineering Pipeline
**Time:** 7:15 PM PST
**Hash:** e716aa2

#### What Was Done
- Created comprehensive feature engineering module (`src/ml/features.py`)
- Implemented all 13+ features: core metrics, derived features, lag features, athlete-specific features
- Built preprocessing module (`src/ml/preprocessing.py`) with scaling, encoding, and time-based splitting
- Created unit tests (`tests/test_features.py`) for all feature functions
- Implemented data leakage prevention - all features only use past data
- Created simple test script for validation
- All tests passed successfully

#### Key Takeaways
- Feature engineering requires careful attention to data leakage - every function must only use data up to the current week, never future data
- Modular design with individual functions for each feature makes testing and debugging much easier
- Time-based data splitting is crucial - random splits would cause data leakage in time series data
- ACWR calculation needs to handle edge cases (zero chronic load, insufficient data) gracefully
- Derived features like ACWR trend and weeks above threshold capture temporal patterns that simple ACWR misses
- Athlete-specific features (age groups, experience) allow personalization of risk assessment
- Preprocessing pipeline should be separate from feature engineering - makes it easier to apply to new data

#### Interview Explanation
**Question:** "How did you engineer features for your injury prediction model while avoiding data leakage?"

**Answer:**
"I built a modular feature engineering pipeline where each feature function explicitly prevents data leakage by only using historical data. For example, when calculating chronic load for week 10, the function only looks at weeks 1-10, never week 11 or beyond. This is critical because in real-world prediction, you'd only have data up to the current week.

I implemented 13+ features organized into categories: core metrics (ACWR, monotony, strain), derived features (ACWR trend, consecutive weeks above threshold), lag features (previous week's ACWR), and athlete-specific features (age groups, experience levels). Each feature function takes the dataset, athlete ID, and week number, then calculates the feature using only data up to that week.

The key insight is that time-based features require time-based validation. I created a time-based split function that divides data by weeks (train: weeks 1-14, validation: 15-19, test: 20-24) rather than random splitting. This simulates real-world prediction where you train on past data and predict future injuries.

I also built a preprocessing pipeline that handles scaling, encoding categorical features, and missing values. The pipeline is designed to fit on training data and transform validation/test data, preventing any information leakage from the test set into the training process.

All features were unit tested to ensure they calculate correctly and don't use future data. This modular approach makes it easy to add new features or modify existing ones without breaking the entire pipeline."

### Commit #2: Implement Synthetic Data Generator for Phase 1
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created `scripts/generate_training_data.py` - comprehensive data generator class
- Implemented athlete profile generation (age, experience, baseline fitness) with realistic distributions
- Built three training patterns: safe progressive, moderate spikes, aggressive spikes
- Implemented feature calculation pipeline (ACWR, monotony, strain, week-over-week change)
- Added injury risk injection based on research parameters (ACWR thresholds, spike detection)
- Created `requirements.txt` with all project dependencies
- Created `scripts/validate_data.py` for data quality validation
- Set up directory structure (scripts/, data/, notebooks/)

#### Key Takeaways
- Using object-oriented design for the generator makes it easy to configure (number of athletes, weeks, random seed) and test
- The injury risk logic needs to check consecutive weeks of high ACWR, not just single weeks - this matches research findings
- Storing daily loads as comma-separated strings in CSV is more practical than nested structures
- Data validation is crucial - need to verify injury rates (15-30%), ACWR distributions, and that injured athletes actually have higher ACWR values
- Using `df.copy()` when modifying DataFrames prevents unexpected side effects

#### Interview Explanation
**Question:** "How did you generate synthetic training data for your injury prediction model?"

**Answer:**
"I built a Python class-based data generator that creates realistic training datasets based on sports science research. The generator creates athlete profiles with age (18-65), experience (0-25 years), and baseline weekly mileage (correlated with experience). 

Each athlete follows one of three training patterns: safe progressive loading (gradual 3-8% increases), moderate spikes (occasional 10-20% increases), or aggressive spikes (frequent 15-30% increases). The patterns include realistic elements like recovery weeks, natural variation, and noise from missed days or life events.

For each week, I calculate features like ACWR (acute load / chronic load), training monotony (mean / std dev), strain (load × monotony), and week-over-week change. Then I apply injury labeling based on research: if ACWR > 1.5 for 2+ consecutive weeks, there's a 60% chance of injury. If week-over-week change > 20%, there's a 40% chance. If both conditions are met, it's 80% chance.

The generator produces CSV files with 150 athletes × 24 weeks = 3,600 data points. I validate the output to ensure injury rates are 15-30% (matching research) and that injured athletes have higher ACWR values than non-injured ones. This synthetic approach is faster than collecting real data and allows precise control over scenarios while still being grounded in research parameters."

---

## Day Summary - January 12, 2026

### Accomplishments
Today marked significant progress on the Injury Risk Predictor project, completing three major phases:

1. **Phase 0: Research & Planning** - Established the scientific foundation with comprehensive documentation of ACWR research, feature specifications, and user flow diagrams.

2. **Phase 1: Data Strategy & Generation** - Built a robust synthetic data generator that creates realistic training datasets (150 athletes × 24 weeks = 3,600 data points) with injury labels based on sports science research. Fixed data quality issues (missing values) and validated the output matches research expectations.

3. **Phase 2: Exploratory Data Analysis** - Created comprehensive EDA notebook with visualizations and statistical tests that validated the synthetic data matches research patterns (injury rate 16.9%, ACWR distributions, etc.).

4. **Phase 3: Feature Engineering** - Implemented complete feature engineering pipeline with 13+ features, preprocessing module, and time-based data splitting. All features designed to prevent data leakage.

### Key Metrics
- **Commits:** 5 major commits
- **Files Created:** 15+ files (documentation, scripts, modules, notebooks)
- **Lines of Code:** ~2,000+ lines
- **Test Coverage:** Unit tests for all feature functions
- **Data Generated:** 3,600 training data points

### Technical Highlights
- Object-oriented design for data generator (configurable, testable)
- Modular feature engineering (individual functions, easy to test)
- Data leakage prevention (time-based features, time-based splitting)
- Comprehensive validation (data quality, statistical tests, unit tests)
- Error handling (optional dependencies, edge cases)

### Challenges Overcome
- Missing values in CSV output (fixed with sentinel values)
- Jupyter installation/command issues (resolved with `python3 -m jupyterlab`)
- Data leakage prevention (careful design of feature functions)
- Testing in sandbox environment (created simple test scripts)

### Next Steps
- Phase 4: Model Development (baseline, Logistic Regression, Random Forest, XGBoost)
- Phase 5: Model Interpretation & Validation
- Phase 6: API Development

---

## Fundamental Knowledge Notes

### Tools & Technologies

#### Jupyter Notebooks
- **What it is:** Interactive computing environment that combines code, visualizations, and markdown documentation
- **Why we use it:** Perfect for exploratory data analysis, iterative development, and creating shareable analysis reports
- **Key concepts:**
  - Cells: Code or markdown blocks that can be executed independently
  - Kernel: The computational engine that executes code
  - Magic commands: Special commands like `%matplotlib inline` for inline plots
- **Usage:** `python3 -m jupyterlab` or `python3 -m notebook` (when `jupyter` command not in PATH)
- **Best practices:** 
  - Use markdown cells for documentation
  - Keep cells focused and modular
  - Save outputs for reproducibility
  - Handle optional dependencies gracefully

#### Pandas
- **What it is:** Python library for data manipulation and analysis
- **Why we use it:** Essential for working with structured data (CSV, DataFrames)
- **Key concepts:**
  - DataFrame: 2D labeled data structure (like Excel spreadsheet)
  - Series: 1D labeled array (single column)
  - Indexing: `.loc[]` for label-based, `.iloc[]` for position-based
  - Groupby: Split-apply-combine operations
  - Rolling windows: `.rolling()` for time-series calculations
- **Common operations:**
  - `pd.read_csv()` - Load data from CSV
  - `df.head()`, `df.describe()` - Explore data
  - `df.groupby()` - Aggregate by groups
  - `df.rolling()` - Calculate rolling statistics
  - `df.fillna()` - Handle missing values
- **Data leakage prevention:** Always use `.iloc[]` or `.loc[]` with proper indexing to avoid future data

#### NumPy
- **What it is:** Fundamental package for numerical computing in Python
- **Why we use it:** Fast array operations, mathematical functions, random number generation
- **Key concepts:**
  - Arrays: N-dimensional arrays (vectors, matrices, tensors)
  - Broadcasting: Operations between arrays of different shapes
  - Vectorization: Operations on entire arrays (faster than loops)
- **Common operations:**
  - `np.array()` - Create arrays
  - `np.random` - Random number generation
  - `np.mean()`, `np.std()` - Statistical functions
  - `np.linspace()`, `np.arange()` - Generate sequences

#### Scikit-learn
- **What it is:** Machine learning library for Python
- **Why we use it:** Provides tools for preprocessing, model training, and evaluation
- **Key concepts:**
  - Preprocessing: `StandardScaler`, `MinMaxScaler`, `LabelEncoder`
  - Models: `LogisticRegression`, `RandomForestClassifier`, etc.
  - Metrics: `accuracy_score`, `precision_score`, `recall_score`, `roc_auc_score`
  - Train/test split: `train_test_split()` (though we use time-based splitting)
- **Best practices:**
  - Fit scalers on training data only
  - Transform validation/test with fitted scaler
  - Use `class_weight='balanced'` for imbalanced datasets

#### Matplotlib & Seaborn
- **What they are:** Python plotting libraries for data visualization
- **Why we use them:** Create publication-quality visualizations for analysis and portfolio
- **Key concepts:**
  - Figure and Axes: Figure is the container, Axes is the plot area
  - Subplots: Multiple plots in one figure
  - Styles: `plt.style.use()` for predefined styles
- **Best practices:**
  - Save at 300 DPI for publication quality
  - Use consistent color schemes
  - Add labels, titles, legends
  - Handle missing seaborn gracefully (fallback to matplotlib)

### Concepts & Methodologies

#### Data Leakage
- **What it is:** When information from the future (test set) leaks into training
- **Why it's critical:** Causes overly optimistic performance estimates, models won't work in production
- **How we prevent it:**
  - Features only use data up to current week
  - Time-based data splitting (not random)
  - Fit scalers on training data only
  - Never use future data in feature calculations
- **Red flags:** Using `df.shift(-1)`, random splits on time-series data, using test set statistics

#### Time-Series Data Splitting
- **What it is:** Splitting data by time periods rather than randomly
- **Why we use it:** Simulates real-world prediction (train on past, predict future)
- **Our approach:**
  - Train: Weeks 1-14 (60%)
  - Validation: Weeks 15-19 (20%)
  - Test: Weeks 20-24 (20%)
- **Alternative:** Time series cross-validation (rolling window)

#### Feature Engineering
- **What it is:** Creating new features from raw data to improve model performance
- **Types of features:**
  - **Core metrics:** Direct calculations (ACWR, monotony)
  - **Derived features:** Combinations or transformations (trends, lags)
  - **Interaction features:** Products of features (ACWR × Age)
  - **Categorical encoding:** Converting categories to numbers
- **Best practices:**
  - Modular functions (one feature per function)
  - Document calculation methods
  - Test edge cases (zero division, missing data)
  - Validate against domain knowledge

#### Synthetic Data Generation
- **What it is:** Creating artificial data that mimics real-world patterns
- **Why we use it:** Faster than collecting real data, controllable, can cite research parameters
- **Key principles:**
  - Base on real research/patterns
  - Include realistic variation and noise
  - Validate distributions match expectations
  - Ensure relationships match domain knowledge
- **Trade-offs:** Faster development, but need to validate it captures real patterns

#### ACWR (Acute:Chronic Workload Ratio)
- **What it is:** Primary metric for injury risk prediction in sports science
- **Calculation:** Acute Load (last 7 days) / Chronic Load (last 28 days average)
- **Risk zones:**
  - < 0.8: Undertrained
  - 0.8-1.3: Sweet spot (lowest risk)
  - 1.3-1.5: Moderate risk
  - > 1.5: High risk (2-4x injury likelihood)
- **Research basis:** Gabbett (2016) - peer-reviewed sports science research

#### Statistical Testing
- **T-test:** Compares means of two groups (injured vs not injured ACWR)
- **Chi-square test:** Tests association between categorical variables (ACWR zones vs injury)
- **Why we use them:** Quantitative validation, not just visual inspection
- **Interpretation:** p-value < 0.05 indicates significant difference/association

### Development Practices

#### Object-Oriented Design
- **What it is:** Organizing code into classes and objects
- **Why we use it:** Makes code reusable, testable, and maintainable
- **Example:** `TrainingDataGenerator` class - configurable, can generate different datasets
- **Benefits:** Encapsulation, reusability, easier testing

#### Unit Testing
- **What it is:** Testing individual functions/components in isolation
- **Why we use it:** Catch bugs early, ensure correctness, document expected behavior
- **Tools:** pytest (preferred) or simple test scripts
- **Best practices:**
  - Test edge cases (zero division, empty data)
  - Test expected behavior
  - Test data leakage prevention
  - Keep tests simple and focused

#### Version Control Best Practices
- **Small, incremental commits:** Easier to review, debug, and rollback
- **Descriptive commit messages:** Explain what and why
- **Test before committing:** Catch issues early
- **Document as you go:** Journal entries, code comments

#### Error Handling
- **What it is:** Gracefully handling unexpected situations
- **Why we use it:** Makes code robust, user-friendly, debuggable
- **Examples:**
  - Try/except for optional dependencies (seaborn)
  - Check for zero division in ACWR calculation
  - Handle missing data gracefully
  - Provide helpful error messages

### Resources & Documentation

#### Research Papers
- **Gabbett (2016):** Established ACWR as injury predictor
- **Hulin (2014):** Week-over-week spikes increase injury risk
- **Soligard (2016):** IOC consensus on load monitoring
- **Why important:** Grounds the project in peer-reviewed science

#### Python Package Management
- **pip3:** Python package installer (use `pip3` not `pip` on macOS)
- **requirements.txt:** Lists all project dependencies
- **Virtual environments:** Isolate project dependencies (best practice, not used here)
- **--user flag:** Install packages for current user only

#### File Organization
- **src/:** Source code (modules, classes)
- **scripts/:** Standalone scripts (data generation, validation)
- **notebooks/:** Jupyter notebooks for exploration
- **data/:** Data files (CSV, models)
- **tests/:** Unit tests
- **docs/:** Documentation
- **outputs/:** Generated visualizations

---

## Transfer Status

✅ **READY FOR GOOGLE DRIVE TRANSFER**

This journal entry is complete and ready to be copied into Google Drive. All commits have been documented with:
- What was done
- Key takeaways
- Interview explanations
- Day summary
- Fundamental knowledge notes

**Date Completed:** January 12, 2026  
**Phases Completed:** Phase 0, Phase 1, Phase 2, Phase 3  
**Total Commits Documented:** 5

---
