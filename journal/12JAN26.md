# Build Journal - January 12, 2026

## Date
January 12, 2026

## Summary
Completed Phase 0 (Research & Planning) by creating comprehensive documentation for the Injury Risk Predictor project, including research summaries, feature engineering specifications, and user flow diagrams. Started Phase 1 (Data Strategy & Generation) by building a synthetic data generator that creates realistic training datasets with injury labels based on sports science research.

---

## Commits & Pushes

### Commit #1: Complete Phase 0 - Research & Planning Documentation
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created `docs/RESEARCH.md` with summaries of 3 core research papers (Gabbett 2016, Hulin 2014, Soligard 2016)
- Documented ACWR methodology, core metrics (ACWR, Monotony, Strain, Week-over-Week Change)
- Created `docs/FEATURES.md` with 13-15 features to engineer, calculation methods, and validation requirements
- Created `docs/USER_FLOW.md` with user journey diagrams, page structures, and error handling flows
- Established foundation for ML problem definition (binary classification: high risk vs low/moderate risk)

#### Key Takeaways
- ACWR (Acute:Chronic Workload Ratio) is the primary injury predictor - when athletes increase training volume too quickly relative to their baseline, injury risk increases 2-4x
- The "sweet spot" for training is ACWR between 0.8-1.3 - this represents optimal progressive overload without excessive risk
- Synthetic data generation is the right approach for MVP - it's faster, controllable, and allows us to cite research parameters
- Feature engineering requires careful attention to data leakage - must only use past data to predict future injuries

#### Interview Explanation
**Question:** "Can you explain the research foundation and approach for your Injury Risk Predictor project?"

**Answer:** 
"The project is based on peer-reviewed sports science research, specifically the Acute:Chronic Workload Ratio (ACWR) concept established by Gabbett in 2016. The core insight is that injury risk increases dramatically when athletes spike their training volume too quickly relative to what they're adapted to. 

I implemented this by calculating ACWR as the ratio of acute load (last 7 days) to chronic load (last 28 days average). Research shows that ACWR between 0.8-1.3 is the 'sweet spot' with lowest injury risk, while values above 1.5 indicate 2-4x increased injury likelihood. 

For the MVP, I chose synthetic data generation over real data because it's faster, allows precise control over injury scenarios, and I can directly cite research parameters. The generator creates 150 athletes over 24 weeks with three training patterns: 30% safe progressive loading, 40% moderate spikes, and 30% aggressive spikes. Injuries are labeled probabilistically based on research: 60% chance if ACWR > 1.5 for 2+ weeks, 40% chance if week-over-week spike > 20%, and 80% chance if both conditions are met.

This approach ensures the dataset matches research distributions (15-30% injury rate) and allows us to validate that the model learns the same patterns that sports scientists have identified."

---

### Commit #3: Fix Missing Values in Data Generator
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Fixed missing values issue in generated CSV files (5,980 missing values)
- Changed `injury_type` from `None` to `'none'` for non-injured athletes
- Changed `injury_week` from `None` to `0` for non-injured athletes
- Added cleanup in `save_data()` method to ensure no NaN values remain
- Regenerated data and validated - all checks pass with no missing values

#### Key Takeaways
- Using `None` in pandas DataFrames creates NaN values which show up as missing data in CSV files
- For categorical data like `injury_type`, using a string value like `'none'` is better than `None`/NaN
- For numeric data like `injury_week`, using `0` as a sentinel value is clearer than NaN
- Always validate data quality after generation - the validation script caught this issue immediately
- Testing before committing is crucial - caught a data quality issue that would have caused problems later

#### Interview Explanation
**Question:** "How did you handle missing values in your synthetic data generation?"

**Answer:**
"Initially, I had a data quality issue where non-injured athletes had `None` values for `injury_type` and `injury_week`, which pandas converted to NaN and showed up as 5,980 missing values in the CSV. 

I fixed this by using explicit sentinel values: `'none'` for `injury_type` and `0` for `injury_week` when an athlete wasn't injured. This approach is cleaner because:
1. It eliminates missing values entirely - the dataset is complete
2. It's more interpretable - `'none'` clearly means no injury, `0` means no injury week
3. It's easier to work with in downstream analysis - no need to handle NaN cases

I also added validation in the `save_data()` method to ensure any remaining NaN values are filled before writing to CSV. The validation script now confirms zero missing values, which is important for machine learning pipelines that expect complete data.

This taught me the importance of testing data quality immediately after generation - catching issues early saves time and prevents problems in later phases."

---

### Commit #4: Complete Phase 2 - Exploratory Data Analysis
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created comprehensive EDA notebook (`notebooks/02_exploratory_analysis.ipynb`)
- Implemented data loading and inspection (missing values, data types, summary statistics)
- Created ACWR distribution analysis with histograms and box plots
- Built injury rate by ACWR zone visualization (bar chart with color coding)
- Implemented training load progression time series (injured vs safe athletes)
- Created correlation analysis with heatmap and scatter plots
- Implemented statistical tests (t-test for ACWR means, chi-square for ACWR zones)
- Added error handling for optional dependencies (seaborn fallback to matplotlib)
- Created outputs directory for saving visualizations
- Documented key insights and findings section

#### Key Takeaways
- EDA is crucial for validating synthetic data matches research expectations - confirmed injury rate (16.9%) is within 15-30% range
- ACWR distribution clearly shows injured athletes have higher ACWR (mean 1.60 vs 0.88) - validates the research foundation
- Statistical tests (t-test, chi-square) provide quantitative confirmation of relationships, not just visual inspection
- Error handling for optional dependencies (seaborn) makes notebooks more portable - users can run even without all packages
- Creating publication-quality visualizations (300 DPI) early helps with portfolio documentation
- Time series plots showing injured vs safe athletes are powerful for explaining the concept to non-technical audiences
- Correlation heatmaps reveal multicollinearity issues early (e.g., acute_load and weekly_load are highly correlated)

#### Interview Explanation
**Question:** "How did you validate your synthetic data and ensure it was suitable for machine learning?"

**Answer:**
"I performed comprehensive exploratory data analysis to validate the synthetic data matched research expectations and was ready for modeling. First, I checked data quality - confirmed no missing values, verified data types, and calculated summary statistics to ensure distributions were reasonable.

Then I validated the core research hypothesis: that high ACWR correlates with injury risk. I created visualizations showing ACWR distributions for injured vs non-injured athletes, which clearly showed injured athletes had higher ACWR values (mean 1.60 vs 0.88). I also calculated injury rates by ACWR zone and confirmed the 'sweet spot' (0.8-1.3) had the lowest injury rate, while high risk zones (>1.5) had significantly higher rates.

I performed statistical tests to quantify these relationships: a t-test confirmed the difference in ACWR means was statistically significant, and a chi-square test confirmed injury rates differed significantly across ACWR zones. This quantitative validation was important - not just visual inspection.

I also created correlation analysis to identify feature relationships and potential multicollinearity issues. Time series plots comparing injured vs safe athletes helped visualize how training load spikes precede injuries.

The EDA confirmed the data was ready for feature engineering - injury rates matched research (16.9% within 15-30% range), ACWR distributions were realistic, and clear predictive relationships existed. This validation step is critical when using synthetic data - you need to ensure it captures the real-world patterns you're trying to model."

---

### Commit #5: Complete Phase 3 - Feature Engineering Pipeline
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created comprehensive feature engineering module (`src/ml/features.py`)
- Implemented all 13+ features: core metrics, derived features, lag features, athlete-specific features
- Built preprocessing module (`src/ml/preprocessing.py`) with scaling, encoding, and time-based splitting
- Created unit tests (`tests/test_features.py`) for all feature functions
- Implemented data leakage prevention - all features only use past data
- Created simple test script for validation
- All tests passed successfully

#### Key Takeaways
- Feature engineering requires careful attention to data leakage - every function must only use data up to the current week, never future data
- Modular design with individual functions for each feature makes testing and debugging much easier
- Time-based data splitting is crucial - random splits would cause data leakage in time series data
- ACWR calculation needs to handle edge cases (zero chronic load, insufficient data) gracefully
- Derived features like ACWR trend and weeks above threshold capture temporal patterns that simple ACWR misses
- Athlete-specific features (age groups, experience) allow personalization of risk assessment
- Preprocessing pipeline should be separate from feature engineering - makes it easier to apply to new data

#### Interview Explanation
**Question:** "How did you engineer features for your injury prediction model while avoiding data leakage?"

**Answer:**
"I built a modular feature engineering pipeline where each feature function explicitly prevents data leakage by only using historical data. For example, when calculating chronic load for week 10, the function only looks at weeks 1-10, never week 11 or beyond. This is critical because in real-world prediction, you'd only have data up to the current week.

I implemented 13+ features organized into categories: core metrics (ACWR, monotony, strain), derived features (ACWR trend, consecutive weeks above threshold), lag features (previous week's ACWR), and athlete-specific features (age groups, experience levels). Each feature function takes the dataset, athlete ID, and week number, then calculates the feature using only data up to that week.

The key insight is that time-based features require time-based validation. I created a time-based split function that divides data by weeks (train: weeks 1-14, validation: 15-19, test: 20-24) rather than random splitting. This simulates real-world prediction where you train on past data and predict future injuries.

I also built a preprocessing pipeline that handles scaling, encoding categorical features, and missing values. The pipeline is designed to fit on training data and transform validation/test data, preventing any information leakage from the test set into the training process.

All features were unit tested to ensure they calculate correctly and don't use future data. This modular approach makes it easy to add new features or modify existing ones without breaking the entire pipeline."

### Commit #2: Implement Synthetic Data Generator for Phase 1
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created `scripts/generate_training_data.py` - comprehensive data generator class
- Implemented athlete profile generation (age, experience, baseline fitness) with realistic distributions
- Built three training patterns: safe progressive, moderate spikes, aggressive spikes
- Implemented feature calculation pipeline (ACWR, monotony, strain, week-over-week change)
- Added injury risk injection based on research parameters (ACWR thresholds, spike detection)
- Created `requirements.txt` with all project dependencies
- Created `scripts/validate_data.py` for data quality validation
- Set up directory structure (scripts/, data/, notebooks/)

#### Key Takeaways
- Using object-oriented design for the generator makes it easy to configure (number of athletes, weeks, random seed) and test
- The injury risk logic needs to check consecutive weeks of high ACWR, not just single weeks - this matches research findings
- Storing daily loads as comma-separated strings in CSV is more practical than nested structures
- Data validation is crucial - need to verify injury rates (15-30%), ACWR distributions, and that injured athletes actually have higher ACWR values
- Using `df.copy()` when modifying DataFrames prevents unexpected side effects

#### Interview Explanation
**Question:** "How did you generate synthetic training data for your injury prediction model?"

**Answer:**
"I built a Python class-based data generator that creates realistic training datasets based on sports science research. The generator creates athlete profiles with age (18-65), experience (0-25 years), and baseline weekly mileage (correlated with experience). 

Each athlete follows one of three training patterns: safe progressive loading (gradual 3-8% increases), moderate spikes (occasional 10-20% increases), or aggressive spikes (frequent 15-30% increases). The patterns include realistic elements like recovery weeks, natural variation, and noise from missed days or life events.

For each week, I calculate features like ACWR (acute load / chronic load), training monotony (mean / std dev), strain (load × monotony), and week-over-week change. Then I apply injury labeling based on research: if ACWR > 1.5 for 2+ consecutive weeks, there's a 60% chance of injury. If week-over-week change > 20%, there's a 40% chance. If both conditions are met, it's 80% chance.

The generator produces CSV files with 150 athletes × 24 weeks = 3,600 data points. I validate the output to ensure injury rates are 15-30% (matching research) and that injured athletes have higher ACWR values than non-injured ones. This synthetic approach is faster than collecting real data and allows precise control over scenarios while still being grounded in research parameters."

---
