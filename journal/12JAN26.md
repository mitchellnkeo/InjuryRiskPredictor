# Build Journal - January 12, 2026

## Date
January 12, 2026

## Summary
Completed Phase 0 (Research & Planning) by creating comprehensive documentation for the Injury Risk Predictor project, including research summaries, feature engineering specifications, and user flow diagrams. Started Phase 1 (Data Strategy & Generation) by building a synthetic data generator that creates realistic training datasets with injury labels based on sports science research.

---

## Commits & Pushes

### Commit #1: Complete Phase 0 - Research & Planning Documentation
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created `docs/RESEARCH.md` with summaries of 3 core research papers (Gabbett 2016, Hulin 2014, Soligard 2016)
- Documented ACWR methodology, core metrics (ACWR, Monotony, Strain, Week-over-Week Change)
- Created `docs/FEATURES.md` with 13-15 features to engineer, calculation methods, and validation requirements
- Created `docs/USER_FLOW.md` with user journey diagrams, page structures, and error handling flows
- Established foundation for ML problem definition (binary classification: high risk vs low/moderate risk)

#### Key Takeaways
- ACWR (Acute:Chronic Workload Ratio) is the primary injury predictor - when athletes increase training volume too quickly relative to their baseline, injury risk increases 2-4x
- The "sweet spot" for training is ACWR between 0.8-1.3 - this represents optimal progressive overload without excessive risk
- Synthetic data generation is the right approach for MVP - it's faster, controllable, and allows us to cite research parameters
- Feature engineering requires careful attention to data leakage - must only use past data to predict future injuries

#### Interview Explanation
**Question:** "Can you explain the research foundation and approach for your Injury Risk Predictor project?"

**Answer:** 
"The project is based on peer-reviewed sports science research, specifically the Acute:Chronic Workload Ratio (ACWR) concept established by Gabbett in 2016. The core insight is that injury risk increases dramatically when athletes spike their training volume too quickly relative to what they're adapted to. 

I implemented this by calculating ACWR as the ratio of acute load (last 7 days) to chronic load (last 28 days average). Research shows that ACWR between 0.8-1.3 is the 'sweet spot' with lowest injury risk, while values above 1.5 indicate 2-4x increased injury likelihood. 

For the MVP, I chose synthetic data generation over real data because it's faster, allows precise control over injury scenarios, and I can directly cite research parameters. The generator creates 150 athletes over 24 weeks with three training patterns: 30% safe progressive loading, 40% moderate spikes, and 30% aggressive spikes. Injuries are labeled probabilistically based on research: 60% chance if ACWR > 1.5 for 2+ weeks, 40% chance if week-over-week spike > 20%, and 80% chance if both conditions are met.

This approach ensures the dataset matches research distributions (15-30% injury rate) and allows us to validate that the model learns the same patterns that sports scientists have identified."

---

### Commit #3: Fix Missing Values in Data Generator
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Fixed missing values issue in generated CSV files (5,980 missing values)
- Changed `injury_type` from `None` to `'none'` for non-injured athletes
- Changed `injury_week` from `None` to `0` for non-injured athletes
- Added cleanup in `save_data()` method to ensure no NaN values remain
- Regenerated data and validated - all checks pass with no missing values

#### Key Takeaways
- Using `None` in pandas DataFrames creates NaN values which show up as missing data in CSV files
- For categorical data like `injury_type`, using a string value like `'none'` is better than `None`/NaN
- For numeric data like `injury_week`, using `0` as a sentinel value is clearer than NaN
- Always validate data quality after generation - the validation script caught this issue immediately
- Testing before committing is crucial - caught a data quality issue that would have caused problems later

#### Interview Explanation
**Question:** "How did you handle missing values in your synthetic data generation?"

**Answer:**
"Initially, I had a data quality issue where non-injured athletes had `None` values for `injury_type` and `injury_week`, which pandas converted to NaN and showed up as 5,980 missing values in the CSV. 

I fixed this by using explicit sentinel values: `'none'` for `injury_type` and `0` for `injury_week` when an athlete wasn't injured. This approach is cleaner because:
1. It eliminates missing values entirely - the dataset is complete
2. It's more interpretable - `'none'` clearly means no injury, `0` means no injury week
3. It's easier to work with in downstream analysis - no need to handle NaN cases

I also added validation in the `save_data()` method to ensure any remaining NaN values are filled before writing to CSV. The validation script now confirms zero missing values, which is important for machine learning pipelines that expect complete data.

This taught me the importance of testing data quality immediately after generation - catching issues early saves time and prevents problems in later phases."

---

### Commit #2: Implement Synthetic Data Generator for Phase 1
**Time:** [To be filled after commit]
**Hash:** [To be filled after commit]

#### What Was Done
- Created `scripts/generate_training_data.py` - comprehensive data generator class
- Implemented athlete profile generation (age, experience, baseline fitness) with realistic distributions
- Built three training patterns: safe progressive, moderate spikes, aggressive spikes
- Implemented feature calculation pipeline (ACWR, monotony, strain, week-over-week change)
- Added injury risk injection based on research parameters (ACWR thresholds, spike detection)
- Created `requirements.txt` with all project dependencies
- Created `scripts/validate_data.py` for data quality validation
- Set up directory structure (scripts/, data/, notebooks/)

#### Key Takeaways
- Using object-oriented design for the generator makes it easy to configure (number of athletes, weeks, random seed) and test
- The injury risk logic needs to check consecutive weeks of high ACWR, not just single weeks - this matches research findings
- Storing daily loads as comma-separated strings in CSV is more practical than nested structures
- Data validation is crucial - need to verify injury rates (15-30%), ACWR distributions, and that injured athletes actually have higher ACWR values
- Using `df.copy()` when modifying DataFrames prevents unexpected side effects

#### Interview Explanation
**Question:** "How did you generate synthetic training data for your injury prediction model?"

**Answer:**
"I built a Python class-based data generator that creates realistic training datasets based on sports science research. The generator creates athlete profiles with age (18-65), experience (0-25 years), and baseline weekly mileage (correlated with experience). 

Each athlete follows one of three training patterns: safe progressive loading (gradual 3-8% increases), moderate spikes (occasional 10-20% increases), or aggressive spikes (frequent 15-30% increases). The patterns include realistic elements like recovery weeks, natural variation, and noise from missed days or life events.

For each week, I calculate features like ACWR (acute load / chronic load), training monotony (mean / std dev), strain (load × monotony), and week-over-week change. Then I apply injury labeling based on research: if ACWR > 1.5 for 2+ consecutive weeks, there's a 60% chance of injury. If week-over-week change > 20%, there's a 40% chance. If both conditions are met, it's 80% chance.

The generator produces CSV files with 150 athletes × 24 weeks = 3,600 data points. I validate the output to ensure injury rates are 15-30% (matching research) and that injured athletes have higher ACWR values than non-injured ones. This synthetic approach is faster than collecting real data and allows precise control over scenarios while still being grounded in research parameters."

---
